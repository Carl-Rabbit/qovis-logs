23/11/10 17:21:03 INFO SparkContext: Running Spark version 3.3.0
23/11/10 17:21:03 DEBUG SecurityUtil: Setting hadoop.security.token.service.use_ip to true
23/11/10 17:21:03 DEBUG Groups:  Creating new Groups object
23/11/10 17:21:03 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
23/11/10 17:21:03 DEBUG NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
23/11/10 17:21:03 DEBUG NativeCodeLoader: java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
23/11/10 17:21:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/10 17:21:03 DEBUG PerformanceAdvisory: Falling back to shell based
23/11/10 17:21:03 DEBUG JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
23/11/10 17:21:03 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
23/11/10 17:21:03 DEBUG UserGroupInformation: Hadoop login
23/11/10 17:21:03 DEBUG UserGroupInformation: hadoop login commit
23/11/10 17:21:03 DEBUG UserGroupInformation: Using local user: UnixPrincipal: dbgroup
23/11/10 17:21:03 DEBUG UserGroupInformation: Using user: "UnixPrincipal: dbgroup" with name: dbgroup
23/11/10 17:21:03 DEBUG UserGroupInformation: User entry: "dbgroup"
23/11/10 17:21:03 DEBUG UserGroupInformation: UGI loginUser: dbgroup (auth:SIMPLE)
23/11/10 17:21:03 INFO ResourceUtils: ==============================================================
23/11/10 17:21:03 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/10 17:21:03 INFO ResourceUtils: ==============================================================
23/11/10 17:21:03 INFO SparkContext: Submitted application: Bug1
23/11/10 17:21:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/10 17:21:03 INFO ResourceProfile: Limiting resource is cpu
23/11/10 17:21:03 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/10 17:21:03 INFO SecurityManager: Changing view acls to: dbgroup
23/11/10 17:21:03 INFO SecurityManager: Changing modify acls to: dbgroup
23/11/10 17:21:03 INFO SecurityManager: Changing view acls groups to: 
23/11/10 17:21:03 INFO SecurityManager: Changing modify acls groups to: 
23/11/10 17:21:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dbgroup); groups with view permissions: Set(); users  with modify permissions: Set(dbgroup); groups with modify permissions: Set()
23/11/10 17:21:03 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework
23/11/10 17:21:03 DEBUG InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
23/11/10 17:21:03 DEBUG InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
23/11/10 17:21:03 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 40
23/11/10 17:21:03 DEBUG PlatformDependent0: -Dio.netty.noUnsafe: false
23/11/10 17:21:03 DEBUG PlatformDependent0: Java version: 8
23/11/10 17:21:03 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available
23/11/10 17:21:03 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available
23/11/10 17:21:03 DEBUG PlatformDependent0: java.nio.Buffer.address: available
23/11/10 17:21:03 DEBUG PlatformDependent0: direct buffer constructor: available
23/11/10 17:21:03 DEBUG PlatformDependent0: java.nio.Bits.unaligned: available, true
23/11/10 17:21:03 DEBUG PlatformDependent0: jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9
23/11/10 17:21:03 DEBUG PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available
23/11/10 17:21:03 DEBUG PlatformDependent: sun.misc.Unsafe: available
23/11/10 17:21:03 DEBUG PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)
23/11/10 17:21:03 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)
23/11/10 17:21:03 DEBUG PlatformDependent: -Dio.netty.maxDirectMemory: 1007157248 bytes
23/11/10 17:21:03 DEBUG PlatformDependent: -Dio.netty.uninitializedArrayAllocationThreshold: -1
23/11/10 17:21:03 DEBUG CleanerJava6: java.nio.ByteBuffer.cleaner(): available
23/11/10 17:21:03 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false
23/11/10 17:21:03 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false
23/11/10 17:21:03 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512
23/11/10 17:21:03 DEBUG PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@63fd4873
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@70e0accd
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@7957dc72
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@6ab72419
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@3aacf32a
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@4fdfa676
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@82c57b3
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@5be82d43
23/11/10 17:21:03 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.level: simple
23/11/10 17:21:03 DEBUG ResourceLeakDetector: -Dio.netty.leakDetection.targetRecords: 4
23/11/10 17:21:03 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 9
23/11/10 17:21:03 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 10
23/11/10 17:21:03 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192
23/11/10 17:21:03 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 11
23/11/10 17:21:03 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 16777216
23/11/10 17:21:03 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256
23/11/10 17:21:03 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64
23/11/10 17:21:03 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768
23/11/10 17:21:03 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192
23/11/10 17:21:03 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimIntervalMillis: 0
23/11/10 17:21:03 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.useCacheForAllThreads: true
23/11/10 17:21:03 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@320e400
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@1cfd1875
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@28c0b664
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@2c444798
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@1af7f54a
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@6ebd78d1
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@436390f4
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@4d157787
23/11/10 17:21:03 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@68ed96ca
23/11/10 17:21:03 DEBUG DefaultChannelId: -Dio.netty.processId: 921481 (auto-detected)
23/11/10 17:21:03 DEBUG NetUtil: -Djava.net.preferIPv4Stack: false
23/11/10 17:21:03 DEBUG NetUtil: -Djava.net.preferIPv6Addresses: false
23/11/10 17:21:03 DEBUG NetUtilInitializations: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)
23/11/10 17:21:03 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 4096
23/11/10 17:21:03 DEBUG DefaultChannelId: -Dio.netty.machineId: 6c:6a:77:ff:fe:47:9c:ad (auto-detected)
23/11/10 17:21:03 DEBUG ByteBufUtil: -Dio.netty.allocator.type: pooled
23/11/10 17:21:03 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 0
23/11/10 17:21:03 DEBUG ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384
23/11/10 17:21:03 DEBUG TransportServer: Shuffle server started on port: 41821
23/11/10 17:21:03 INFO Utils: Successfully started service 'sparkDriver' on port 41821.
23/11/10 17:21:03 DEBUG SparkEnv: Using serializer: class org.apache.spark.serializer.JavaSerializer
23/11/10 17:21:03 INFO SparkEnv: Registering MapOutputTracker
23/11/10 17:21:03 DEBUG MapOutputTrackerMasterEndpoint: init
23/11/10 17:21:03 INFO SparkEnv: Registering BlockManagerMaster
23/11/10 17:21:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/10 17:21:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/10 17:21:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/10 17:21:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d936ce8a-840e-42b2-8cf2-462f418d5214
23/11/10 17:21:03 DEBUG DiskBlockManager: Adding shutdown hook
23/11/10 17:21:04 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
23/11/10 17:21:04 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/10 17:21:04 DEBUG OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: init
23/11/10 17:21:04 DEBUG SecurityManager: Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
23/11/10 17:21:04 DEBUG JettyUtils: Using requestHeaderSize: 8192
23/11/10 17:21:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/10 17:21:04 INFO SparkContext: Added JAR file:/home/dbgroup/Documents/Code/Research/qotrace/qotrace-misc/spark-program/bug1/target/scala-2.12/bug1-assembly-1.0.0.jar at spark://Jiahao:41821/jars/bug1-assembly-1.0.0.jar with timestamp 1699608063673
23/11/10 17:21:04 TRACE HeartbeatReceiver: Checking for hosts with no recent heartbeats in HeartbeatReceiver.
23/11/10 17:21:04 INFO Executor: Starting executor ID driver on host Jiahao
23/11/10 17:21:04 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/10 17:21:04 INFO Executor: Fetching spark://Jiahao:41821/jars/bug1-assembly-1.0.0.jar with timestamp 1699608063673
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@acf859d
23/11/10 17:21:04 TRACE TransportClientFactory: DNS resolution succeed for Jiahao/127.0.1.1:41821 took 0 ms
23/11/10 17:21:04 DEBUG TransportClientFactory: Creating new connection to Jiahao/127.0.1.1:41821
23/11/10 17:21:04 DEBUG AbstractByteBuf: -Dio.netty.buffer.checkAccessible: true
23/11/10 17:21:04 DEBUG AbstractByteBuf: -Dio.netty.buffer.checkBounds: true
23/11/10 17:21:04 DEBUG ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@7bfebe73
23/11/10 17:21:04 DEBUG TransportClientFactory: Connection to Jiahao/127.0.1.1:41821 successful, running bootstraps...
23/11/10 17:21:04 INFO TransportClientFactory: Successfully created connection to Jiahao/127.0.1.1:41821 after 15 ms (0 ms spent in bootstraps)
23/11/10 17:21:04 DEBUG TransportClient: Sending stream request for /jars/bug1-assembly-1.0.0.jar to Jiahao/127.0.1.1:41821
23/11/10 17:21:04 DEBUG TransportServer: New connection accepted for remote address /127.0.0.1:35378.
23/11/10 17:21:04 DEBUG Recycler: -Dio.netty.recycler.maxCapacityPerThread: 4096
23/11/10 17:21:04 DEBUG Recycler: -Dio.netty.recycler.ratio: 8
23/11/10 17:21:04 DEBUG Recycler: -Dio.netty.recycler.chunkSize: 32
23/11/10 17:21:04 DEBUG Recycler: -Dio.netty.recycler.blocking: false
23/11/10 17:21:04 INFO Utils: Fetching spark://Jiahao:41821/jars/bug1-assembly-1.0.0.jar to /tmp/spark-d66f7738-29c1-4a72-86f6-041587eb6520/userFiles-4bc751c7-bffa-4e4b-8c80-8caac533d4c6/fetchFileTemp9192407590105241415.tmp
23/11/10 17:21:04 TRACE TransportClient: Sending request /jars/bug1-assembly-1.0.0.jar to Jiahao/127.0.1.1:41821 took 7 ms
23/11/10 17:21:04 TRACE MessageDecoder: Received message StreamRequest: StreamRequest[streamId=/jars/bug1-assembly-1.0.0.jar]
23/11/10 17:21:04 TRACE TransportRequestHandler: Received req from /127.0.0.1:35378 to fetch stream /jars/bug1-assembly-1.0.0.jar
23/11/10 17:21:04 TRACE MessageDecoder: Received message StreamResponse: StreamResponse[streamId=/jars/bug1-assembly-1.0.0.jar,byteCount=5545597,body=<null>]
23/11/10 17:21:04 TRACE TransportRequestHandler: Sent result StreamResponse[streamId=/jars/bug1-assembly-1.0.0.jar,byteCount=5545597,body=FileSegmentManagedBuffer[file=/home/dbgroup/Documents/Code/Research/qotrace/qotrace-misc/spark-program/bug1/target/scala-2.12/bug1-assembly-1.0.0.jar,offset=0,length=5545597]] to client /127.0.0.1:35378
23/11/10 17:21:04 INFO Executor: Adding file:/tmp/spark-d66f7738-29c1-4a72-86f6-041587eb6520/userFiles-4bc751c7-bffa-4e4b-8c80-8caac533d4c6/bug1-assembly-1.0.0.jar to class loader
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@681adc8f
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@3506d826
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@35dd9ed3
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@8ff5094
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@363f0ba0
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@35fb22a9
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@6c8909c3
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@1e008f36
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@3701e6e4
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@65f58c6e
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@73ad7e90
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@4ba380c7
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@784c5ef5
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@31133b6e
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@29528a22
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@7ea08277
23/11/10 17:21:04 TRACE NioEventLoop: instrumented a special java.util.Set into: sun.nio.ch.EPollSelectorImpl@6401d0a0
23/11/10 17:21:04 DEBUG TransportServer: Shuffle server started on port: 37589
23/11/10 17:21:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37589.
23/11/10 17:21:04 INFO NettyBlockTransferService: Server created on Jiahao:37589
23/11/10 17:21:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/10 17:21:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, Jiahao, 37589, None)
23/11/10 17:21:04 DEBUG DefaultTopologyMapper: Got a request for Jiahao
23/11/10 17:21:04 INFO BlockManagerMasterEndpoint: Registering block manager Jiahao:37589 with 366.3 MiB RAM, BlockManagerId(driver, Jiahao, 37589, None)
23/11/10 17:21:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, Jiahao, 37589, None)
23/11/10 17:21:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, Jiahao, 37589, None)
23/11/10 17:21:04 DEBUG FileSystem: Starting: Acquiring creator semaphore for hdfs://Jiahao:9000/spark3.3.0-logs
23/11/10 17:21:04 DEBUG FileSystem: Acquiring creator semaphore for hdfs://Jiahao:9000/spark3.3.0-logs: duration 0:00.001s
23/11/10 17:21:04 DEBUG FileSystem: Starting: Creating FS hdfs://Jiahao:9000/spark3.3.0-logs
23/11/10 17:21:04 DEBUG FileSystem: Loading filesystems
23/11/10 17:21:04 DEBUG FileSystem: nullscan:// = class org.apache.hadoop.hive.ql.io.NullScanFileSystem from /home/dbgroup/Applications/spark-3.3.0/jars/hive-exec-2.3.9-core.jar
23/11/10 17:21:04 DEBUG FileSystem: file:// = class org.apache.hadoop.fs.LocalFileSystem from /home/dbgroup/Applications/spark-3.3.0/jars/hadoop-client-api-3.3.2.jar
23/11/10 17:21:04 DEBUG FileSystem: file:// = class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem from /home/dbgroup/Applications/spark-3.3.0/jars/hive-exec-2.3.9-core.jar
23/11/10 17:21:04 DEBUG FileSystem: viewfs:// = class org.apache.hadoop.fs.viewfs.ViewFileSystem from /home/dbgroup/Applications/spark-3.3.0/jars/hadoop-client-api-3.3.2.jar
23/11/10 17:21:04 DEBUG FileSystem: har:// = class org.apache.hadoop.fs.HarFileSystem from /home/dbgroup/Applications/spark-3.3.0/jars/hadoop-client-api-3.3.2.jar
23/11/10 17:21:04 DEBUG FileSystem: http:// = class org.apache.hadoop.fs.http.HttpFileSystem from /home/dbgroup/Applications/spark-3.3.0/jars/hadoop-client-api-3.3.2.jar
23/11/10 17:21:04 DEBUG FileSystem: https:// = class org.apache.hadoop.fs.http.HttpsFileSystem from /home/dbgroup/Applications/spark-3.3.0/jars/hadoop-client-api-3.3.2.jar
23/11/10 17:21:04 DEBUG FileSystem: hdfs:// = class org.apache.hadoop.hdfs.DistributedFileSystem from /home/dbgroup/Applications/spark-3.3.0/jars/hadoop-client-api-3.3.2.jar
23/11/10 17:21:04 DEBUG FileSystem: webhdfs:// = class org.apache.hadoop.hdfs.web.WebHdfsFileSystem from /home/dbgroup/Applications/spark-3.3.0/jars/hadoop-client-api-3.3.2.jar
23/11/10 17:21:04 DEBUG FileSystem: swebhdfs:// = class org.apache.hadoop.hdfs.web.SWebHdfsFileSystem from /home/dbgroup/Applications/spark-3.3.0/jars/hadoop-client-api-3.3.2.jar
23/11/10 17:21:04 DEBUG FileSystem: Looking for FS supporting hdfs
23/11/10 17:21:04 DEBUG FileSystem: looking for configuration option fs.hdfs.impl
23/11/10 17:21:04 DEBUG FileSystem: Looking in service filesystems for implementation class
23/11/10 17:21:04 DEBUG FileSystem: FS for hdfs is class org.apache.hadoop.hdfs.DistributedFileSystem
23/11/10 17:21:04 DEBUG DfsClientConf: dfs.client.use.legacy.blockreader.local = false
23/11/10 17:21:04 DEBUG DfsClientConf: dfs.client.read.shortcircuit = false
23/11/10 17:21:04 DEBUG DfsClientConf: dfs.client.domain.socket.data.traffic = false
23/11/10 17:21:04 DEBUG DfsClientConf: dfs.domain.socket.path = 
23/11/10 17:21:04 DEBUG DFSClient: Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
23/11/10 17:21:04 TRACE SecurityUtil: Name lookup for Jiahao took 0 ms.
23/11/10 17:21:04 DEBUG RetryUtils: multipleLinearRandomRetry = null
23/11/10 17:21:04 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker@6a0094c9
23/11/10 17:21:04 DEBUG Client: getting client out of cache: Client-ad2a875726b643d9bb1d0cbc48609258
23/11/10 17:21:04 DEBUG PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.
23/11/10 17:21:04 DEBUG DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
23/11/10 17:21:04 DEBUG FileSystem: Creating FS hdfs://Jiahao:9000/spark3.3.0-logs: duration 0:00.252s
23/11/10 17:21:04 TRACE SecurityUtil: Name lookup for Jiahao took 0 ms.
23/11/10 17:21:04 TRACE ProtobufRpcEngine2: 1: Call -> Jiahao/127.0.1.1:9000: getFileInfo {src: "/spark3.3.0-logs"}
23/11/10 17:21:04 DEBUG Client: The ping interval is 60000 ms.
23/11/10 17:21:04 DEBUG Client: Connecting to Jiahao/127.0.1.1:9000
23/11/10 17:21:04 DEBUG Client: Setup connection to Jiahao/127.0.1.1:9000
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup: starting, having connections 1
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup got value #0
23/11/10 17:21:04 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 62ms
23/11/10 17:21:04 TRACE ProtobufRpcEngine2: 1: Response <- Jiahao/127.0.1.1:9000: getFileInfo {fs { fileType: IS_DIR path: "" length: 0 permission { perm: 493 } owner: "dbgroup" group: "supergroup" modification_time: 1699608062450 access_time: 0 block_replication: 0 blocksize: 0 fileId: 16394 childrenNum: 803 storagePolicy: 0 flags: 0 }}
23/11/10 17:21:04 DEBUG DFSClient: /spark3.3.0-logs: masked={ masked: rwxr-xr-x, unmasked: rwxrwxrwx }
23/11/10 17:21:04 TRACE ProtobufRpcEngine2: 1: Call -> Jiahao/127.0.1.1:9000: mkdirs {src: "/spark3.3.0-logs" masked { perm: 493 } createParent: true unmasked { perm: 511 }}
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup got value #1
23/11/10 17:21:04 DEBUG ProtobufRpcEngine2: Call: mkdirs took 1ms
23/11/10 17:21:04 TRACE ProtobufRpcEngine2: 1: Response <- Jiahao/127.0.1.1:9000: mkdirs {result: true}
23/11/10 17:21:04 DEBUG DFSClient: /spark3.3.0-logs/local-1699608064198.inprogress: masked={ masked: rw-r--r--, unmasked: rw-rw-rw- }
23/11/10 17:21:04 TRACE ProtobufRpcEngine2: 1: Call -> Jiahao/127.0.1.1:9000: create {src: "/spark3.3.0-logs/local-1699608064198.inprogress" masked { perm: 420 } clientName: "DFSClient_NONMAPREDUCE_-784229600_1" createFlag: 131 createParent: false replication: 2 blockSize: 134217728 cryptoProtocolVersion: ENCRYPTION_ZONES unmasked { perm: 438 }}
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.create
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup got value #2
23/11/10 17:21:04 DEBUG ProtobufRpcEngine2: Call: create took 2ms
23/11/10 17:21:04 TRACE ProtobufRpcEngine2: 1: Response <- Jiahao/127.0.1.1:9000: create {fs { fileType: IS_FILE path: "" length: 0 permission { perm: 420 } owner: "dbgroup" group: "supergroup" modification_time: 1699608064773 access_time: 1699608064773 block_replication: 2 blocksize: 134217728 fileId: 17316 childrenNum: 0 storagePolicy: 0 flags: 0 }}
23/11/10 17:21:04 DEBUG DFSClient: computePacketChunkSize: src=/spark3.3.0-logs/local-1699608064198.inprogress, chunkSize=516, chunksPerPacket=126, packetSize=65016
23/11/10 17:21:04 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_-784229600_1] with renew id 1 started
23/11/10 17:21:04 TRACE ProtobufRpcEngine2: 1: Call -> Jiahao/127.0.1.1:9000: setPermission {src: "/spark3.3.0-logs/local-1699608064198.inprogress" permission { perm: 432 }}
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.setPermission
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup got value #3
23/11/10 17:21:04 DEBUG ProtobufRpcEngine2: Call: setPermission took 1ms
23/11/10 17:21:04 TRACE ProtobufRpcEngine2: 1: Response <- Jiahao/127.0.1.1:9000: setPermission {}
23/11/10 17:21:04 INFO SingleEventLogFileWriter: Logging events to hdfs://Jiahao:9000/spark3.3.0-logs/local-1699608064198.inprogress
23/11/10 17:21:04 DEBUG DFSClient: WriteChunk allocating new packet seqno=0, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0, output stream=DFSOutputStream:block==null
23/11/10 17:21:04 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=58, lastFlushOffset=0, createNewBlock=false
23/11/10 17:21:04 DEBUG DataStreamer: Queued packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 58, block==null
23/11/10 17:21:04 DEBUG DataStreamer: block==null waiting for ack for: 0
23/11/10 17:21:04 DEBUG DataStreamer: stage=PIPELINE_SETUP_CREATE, block==null
23/11/10 17:21:04 DEBUG DataStreamer: Allocating new block: block==null
23/11/10 17:21:04 TRACE ProtobufRpcEngine2: 84: Call -> Jiahao/127.0.1.1:9000: addBlock {src: "/spark3.3.0-logs/local-1699608064198.inprogress" clientName: "DFSClient_NONMAPREDUCE_-784229600_1" fileId: 17316}
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup sending #4 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup got value #4
23/11/10 17:21:04 DEBUG ProtobufRpcEngine2: Call: addBlock took 4ms
23/11/10 17:21:04 TRACE ProtobufRpcEngine2: 84: Response <- Jiahao/127.0.1.1:9000: addBlock {block { b { poolId: "BP-80912574-127.0.1.1-1665050837178" blockId: 1073742676 generationStamp: 1857 numBytes: 0 } offset: 0 locs { id { ipAddr: "127.0.0.1" hostName: "Jiahao" datanodeUuid: "76c1a4e3-c95c-492e-a12a-10d1e3b1cff0" xferPort: 9866 infoPort: 9864 ipcPort: 9867 infoSecurePort: 0 } capacity: 490577010688 dfsUsed: 1003272763 remaining: 258510209024 blockPoolUsed: 1003272763 lastUpdate: 1699608062862 xceiverCount: 1 location: "/default-rack" nonDfsUsed: 206068311493 adminState: NORMAL cacheCapacity: 0 cacheUsed: 0 lastUpdateMonotonic: 688518988 lastBlockReportTime: 1699606814805 lastBlockReportMonotonic: 687270931 numBlocks: 0 } corrupt: false blockToken { identifier: "" password: "" kind: "" service: "" } isCached: false storageTypes: DISK storageIDs: "DS-a80b40d4-6a51-4578-b627-a115cddfbe16" }}
23/11/10 17:21:04 DEBUG DataStreamer: pipeline = [DatanodeInfoWithStorage[127.0.0.1:9866,DS-a80b40d4-6a51-4578-b627-a115cddfbe16,DISK]], blk_1073742676_1857
23/11/10 17:21:04 DEBUG DataStreamer: Connecting to datanode 127.0.0.1:9866
23/11/10 17:21:04 TRACE SecurityUtil: Name lookup for 127.0.0.1 took 0 ms.
23/11/10 17:21:04 DEBUG DataStreamer: Send buf size 1313280
23/11/10 17:21:04 DEBUG SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
23/11/10 17:21:04 TRACE ProtobufRpcEngine2: 84: Call -> Jiahao/127.0.1.1:9000: getServerDefaults {}
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup sending #5 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup got value #5
23/11/10 17:21:04 DEBUG ProtobufRpcEngine2: Call: getServerDefaults took 1ms
23/11/10 17:21:04 TRACE ProtobufRpcEngine2: 84: Response <- Jiahao/127.0.1.1:9000: getServerDefaults {serverDefaults { blockSize: 134217728 bytesPerChecksum: 512 writePacketSize: 65536 replication: 2 fileBufferSize: 4096 encryptDataTransfer: false trashInterval: 0 checksumType: CHECKSUM_CRC32C keyProviderUri: "" policyId: 7 }}
23/11/10 17:21:04 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /127.0.0.1, datanodeId = DatanodeInfoWithStorage[127.0.0.1:9866,DS-a80b40d4-6a51-4578-b627-a115cddfbe16,DISK]
23/11/10 17:21:04 TRACE FastByteComparisons: Unsafe comparer selected for byte unaligned system architecture
23/11/10 17:21:04 TRACE DataTransferProtocol: Sending DataTransferOp OpWriteBlockProto: header {
  baseHeader {
    block {
      poolId: "BP-80912574-127.0.1.1-1665050837178"
      blockId: 1073742676
      generationStamp: 1857
      numBytes: 134217728
    }
    token {
      identifier: ""
      password: ""
      kind: ""
      service: ""
    }
  }
  clientName: "DFSClient_NONMAPREDUCE_-784229600_1"
}
stage: PIPELINE_SETUP_CREATE
pipelineSize: 1
minBytesRcvd: 0
maxBytesRcvd: 0
latestGenerationStamp: 0
requestedChecksum {
  type: CHECKSUM_CRC32C
  bytesPerChecksum: 512
}
cachingStrategy {
}
storageType: DISK
allowLazyPersist: false
pinning: false
targetPinnings: false
storageId: "DS-a80b40d4-6a51-4578-b627-a115cddfbe16"

23/11/10 17:21:04 DEBUG DataStreamer: nodes [DatanodeInfoWithStorage[127.0.0.1:9866,DS-a80b40d4-6a51-4578-b627-a115cddfbe16,DISK]] storageTypes [DISK] storageIDs [DS-a80b40d4-6a51-4578-b627-a115cddfbe16]
23/11/10 17:21:04 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 58
23/11/10 17:21:04 DEBUG DataStreamer: DFSClient seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:04 TRACE ProtobufRpcEngine2: 1: Call -> Jiahao/127.0.1.1:9000: fsync {src: "/spark3.3.0-logs/local-1699608064198.inprogress" client: "DFSClient_NONMAPREDUCE_-784229600_1" lastBlockLength: 58 fileId: 17316}
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup sending #6 org.apache.hadoop.hdfs.protocol.ClientProtocol.fsync
23/11/10 17:21:04 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup got value #6
23/11/10 17:21:04 DEBUG ProtobufRpcEngine2: Call: fsync took 1ms
23/11/10 17:21:04 TRACE ProtobufRpcEngine2: 1: Response <- Jiahao/127.0.1.1:9000: fsync {}
23/11/10 17:21:04 DEBUG DFSClient: WriteChunk allocating new packet seqno=1, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:04 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=477, lastFlushOffset=58, createNewBlock=false
23/11/10 17:21:04 DEBUG DataStreamer: Queued packet seqno: 1 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 477, blk_1073742676_1857
23/11/10 17:21:04 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 1
23/11/10 17:21:04 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:04 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 1 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 477
23/11/10 17:21:05 DEBUG DataStreamer: DFSClient seqno: 1 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:05 DEBUG DFSClient: WriteChunk allocating new packet seqno=2, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:05 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=685, lastFlushOffset=477, createNewBlock=false
23/11/10 17:21:05 DEBUG DataStreamer: Queued packet seqno: 2 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 685, blk_1073742676_1857
23/11/10 17:21:05 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 2
23/11/10 17:21:05 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:05 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 2 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 685
23/11/10 17:21:05 DEBUG DataStreamer: DFSClient seqno: 2 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:05 DEBUG DFSClient: WriteChunk allocating new packet seqno=3, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=512, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:05 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=915, lastFlushOffset=685, createNewBlock=false
23/11/10 17:21:05 DEBUG DataStreamer: Queued packet seqno: 3 offsetInBlock: 512 lastPacketInBlock: false lastByteOffsetInBlock: 915, blk_1073742676_1857
23/11/10 17:21:05 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 3
23/11/10 17:21:05 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:05 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 3 offsetInBlock: 512 lastPacketInBlock: false lastByteOffsetInBlock: 915
23/11/10 17:21:05 DEBUG DataStreamer: DFSClient seqno: 3 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:05 DEBUG SparkContext: Adding shutdown hook
23/11/10 17:21:05 DEBUG DFSClient: WriteChunk allocating new packet seqno=4, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=512, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:05 DEBUG DFSOutputStream: enqueue full packet seqno: 4 offsetInBlock: 512 lastPacketInBlock: false lastByteOffsetInBlock: 65024, src=/spark3.3.0-logs/local-1699608064198.inprogress, bytesCurBlock=65024, blockSize=134217728, appendChunk=false, blk_1073742676_1857
23/11/10 17:21:05 DEBUG DataStreamer: Queued packet seqno: 4 offsetInBlock: 512 lastPacketInBlock: false lastByteOffsetInBlock: 65024, blk_1073742676_1857
23/11/10 17:21:05 DEBUG DFSClient: computePacketChunkSize: src=/spark3.3.0-logs/local-1699608064198.inprogress, chunkSize=516, chunksPerPacket=126, packetSize=65016
23/11/10 17:21:05 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:05 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 4 offsetInBlock: 512 lastPacketInBlock: false lastByteOffsetInBlock: 65024
23/11/10 17:21:05 DEBUG DFSClient: WriteChunk allocating new packet seqno=5, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=65024, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:05 DEBUG DataStreamer: DFSClient seqno: 4 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:05 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=87578, lastFlushOffset=915, createNewBlock=false
23/11/10 17:21:05 DEBUG DataStreamer: Queued packet seqno: 5 offsetInBlock: 65024 lastPacketInBlock: false lastByteOffsetInBlock: 87578, blk_1073742676_1857
23/11/10 17:21:05 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 5
23/11/10 17:21:05 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:05 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 5 offsetInBlock: 65024 lastPacketInBlock: false lastByteOffsetInBlock: 87578
23/11/10 17:21:05 DEBUG DataStreamer: DFSClient seqno: 5 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:05 DEBUG FileSystem: Looking for FS supporting file
23/11/10 17:21:05 DEBUG FileSystem: looking for configuration option fs.file.impl
23/11/10 17:21:05 DEBUG FileSystem: Looking in service filesystems for implementation class
23/11/10 17:21:05 DEBUG FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
23/11/10 17:21:05 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
23/11/10 17:21:05 INFO SharedState: Warehouse path is 'hdfs://Jiahao:9000/warehouse'.
23/11/10 17:21:05 DEBUG FsUrlStreamHandlerFactory: Creating handler for protocol jar
23/11/10 17:21:05 DEBUG FileSystem: Looking for FS supporting jar
23/11/10 17:21:05 DEBUG FileSystem: looking for configuration option fs.jar.impl
23/11/10 17:21:05 DEBUG FileSystem: Looking in service filesystems for implementation class
23/11/10 17:21:05 DEBUG FsUrlStreamHandlerFactory: Unknown protocol jar, delegating to default implementation
23/11/10 17:21:05 DEBUG FsUrlStreamHandlerFactory: Creating handler for protocol file
23/11/10 17:21:05 DEBUG FileSystem: Looking for FS supporting file
23/11/10 17:21:05 DEBUG FileSystem: looking for configuration option fs.file.impl
23/11/10 17:21:05 DEBUG FileSystem: Looking in service filesystems for implementation class
23/11/10 17:21:05 DEBUG FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
23/11/10 17:21:05 DEBUG FsUrlStreamHandlerFactory: Found implementation of file: class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem
23/11/10 17:21:05 DEBUG FsUrlStreamHandlerFactory: Using handler for protocol file
23/11/10 17:21:06 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.expressions.codegen.package$ExpressionCanonicalizer$CleanExpressions ===
!input[0, int, false] AS value#0   input[0, int, false]
!+- input[0, int, false]           
           
23/11/10 17:21:06 TRACE package$ExpressionCanonicalizer: Fixed point reached for batch CleanExpressions after 2 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: 
=== Result of Batch CleanExpressions ===
!input[0, int, false] AS value#0   input[0, int, false]
!+- input[0, int, false]           
          
23/11/10 17:21:06 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 2
Total time: 0.0010682 seconds
Total number of effective runs: 1
Total time of effective runs: 8.86258E-4 seconds
      
23/11/10 17:21:06 DEBUG GenerateUnsafeProjection: code for input[0, int, false]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */     int value_0 = i.getInt(0);
/* 032 */     mutableStateArray_0[0].write(0, value_0);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

23/11/10 17:21:06 DEBUG CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */     int value_0 = i.getInt(0);
/* 032 */     mutableStateArray_0[0].write(0, value_0);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

23/11/10 17:21:06 INFO CodeGenerator: Code generated in 109.407144 ms
23/11/10 17:21:06 DEBUG CatalystSqlParser: Parsing command: spark_grouping_id
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Substitution after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Substitution has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Disable Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Disable Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Simple Sanity Check after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Simple Sanity Check has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Keep Legacy Outputs after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Keep Legacy Outputs has no effect.
23/11/10 17:21:06 TRACE Analyzer$ResolveReferences: Attempting to resolve LocalRelation [value#1]
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Resolution after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Resolution has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove TempResolvedColumn after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Remove TempResolvedColumn has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Apply Char Padding after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Apply Char Padding has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Post-Hoc Resolution after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Post-Hoc Resolution has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove Unresolved Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Remove Unresolved Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Nondeterministic after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Nondeterministic has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UDF after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch UDF has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UpdateNullability after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch UpdateNullability has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Subquery after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Subquery has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Cleanup after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Cleanup has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch HandleAnalysisOnlyCommand after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch HandleAnalysisOnlyCommand has no effect.
23/11/10 17:21:06 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 77
Total time: 0.119054809 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Substitution after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Substitution has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Disable Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Disable Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Simple Sanity Check after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Simple Sanity Check has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Keep Legacy Outputs after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Keep Legacy Outputs has no effect.
23/11/10 17:21:06 TRACE Analyzer$ResolveReferences: Attempting to resolve LocalRelation <empty>, [value#1]
23/11/10 17:21:06 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer ===
!'DeserializeToObject unresolveddeserializer(assertnotnull(upcast(getcolumnbyordinal(0, IntegerType), IntegerType, - root class: "scala.Int"))), obj#2: int   'DeserializeToObject assertnotnull(upcast(value#1, IntegerType, - root class: "scala.Int")), obj#2: int
 +- LocalRelation <empty>, [value#1]                                                                                                                          +- LocalRelation <empty>, [value#1]
           
23/11/10 17:21:06 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast ===
!'DeserializeToObject assertnotnull(upcast(value#1, IntegerType, - root class: "scala.Int")), obj#2: int   DeserializeToObject assertnotnull(cast(value#1 as int)), obj#2: int
 +- LocalRelation <empty>, [value#1]                                                                       +- LocalRelation <empty>, [value#1]
           
23/11/10 17:21:06 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.ResolveTimeZone ===
 DeserializeToObject assertnotnull(cast(value#1 as int)), obj#2: int   DeserializeToObject assertnotnull(cast(value#1 as int)), obj#2: int
 +- LocalRelation <empty>, [value#1]                                   +- LocalRelation <empty>, [value#1]
           
23/11/10 17:21:06 TRACE Analyzer$ResolveReferences: Attempting to resolve DeserializeToObject assertnotnull(cast(value#1 as int)), obj#2: int
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Resolution after 2 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(assertnotnull(upcast(getcolumnbyordinal(0, IntegerType), IntegerType, - root class: "scala.Int"))), obj#2: int   DeserializeToObject assertnotnull(cast(value#1 as int)), obj#2: int
 +- LocalRelation <empty>, [value#1]                                                                                                                          +- LocalRelation <empty>, [value#1]
          
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove TempResolvedColumn after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Remove TempResolvedColumn has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Apply Char Padding after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Apply Char Padding has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Post-Hoc Resolution after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Post-Hoc Resolution has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove Unresolved Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Remove Unresolved Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Nondeterministic after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Nondeterministic has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UDF after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch UDF has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UpdateNullability after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch UpdateNullability has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Subquery after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Subquery has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Cleanup after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Cleanup has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch HandleAnalysisOnlyCommand after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch HandleAnalysisOnlyCommand has no effect.
23/11/10 17:21:06 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 128
Total time: 0.0182228 seconds
Total number of effective runs: 3
Total time of effective runs: 0.010493922 seconds
      
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Substitution after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Substitution has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Disable Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Disable Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Simple Sanity Check after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Simple Sanity Check has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Keep Legacy Outputs after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Keep Legacy Outputs has no effect.
23/11/10 17:21:06 TRACE Analyzer$ResolveReferences: Attempting to resolve Project [value#1 AS id#4]
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Resolution after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Resolution has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove TempResolvedColumn after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Remove TempResolvedColumn has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Apply Char Padding after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Apply Char Padding has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Post-Hoc Resolution after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Post-Hoc Resolution has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove Unresolved Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Remove Unresolved Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Nondeterministic after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Nondeterministic has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UDF after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch UDF has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UpdateNullability after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch UpdateNullability has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Subquery after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Subquery has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Cleanup after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Cleanup has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch HandleAnalysisOnlyCommand after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch HandleAnalysisOnlyCommand has no effect.
23/11/10 17:21:06 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 77
Total time: 0.003132901 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Substitution after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Substitution has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Disable Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Disable Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Simple Sanity Check after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Simple Sanity Check has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Keep Legacy Outputs after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Keep Legacy Outputs has no effect.
23/11/10 17:21:06 TRACE Analyzer$ResolveReferences: Attempting to resolve Deduplicate [id#4]
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Resolution after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Resolution has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove TempResolvedColumn after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Remove TempResolvedColumn has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Apply Char Padding after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Apply Char Padding has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Post-Hoc Resolution after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Post-Hoc Resolution has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove Unresolved Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Remove Unresolved Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Nondeterministic after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Nondeterministic has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UDF after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch UDF has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UpdateNullability after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch UpdateNullability has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Subquery after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Subquery has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Cleanup after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Cleanup has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch HandleAnalysisOnlyCommand after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch HandleAnalysisOnlyCommand has no effect.
23/11/10 17:21:06 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 77
Total time: 0.002965221 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:06 DEBUG SparkSqlParser: Parsing command: ids
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Substitution after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Substitution has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Disable Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Disable Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Simple Sanity Check after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Simple Sanity Check has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Keep Legacy Outputs after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Keep Legacy Outputs has no effect.
23/11/10 17:21:06 TRACE Analyzer$ResolveReferences: Attempting to resolve CreateViewCommand `ids`, false, true, LocalTempView, true
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Resolution after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Resolution has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove TempResolvedColumn after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Remove TempResolvedColumn has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Apply Char Padding after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Apply Char Padding has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Post-Hoc Resolution after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Post-Hoc Resolution has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove Unresolved Hints after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Remove Unresolved Hints has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Nondeterministic after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Nondeterministic has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UDF after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch UDF has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UpdateNullability after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch UpdateNullability has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Subquery after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Subquery has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Cleanup after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch Cleanup has no effect.
23/11/10 17:21:06 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch HandleAnalysisOnlyCommand after 1 iterations.
23/11/10 17:21:06 TRACE PlanChangeLogger: Batch HandleAnalysisOnlyCommand has no effect.
23/11/10 17:21:06 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 77
Total time: 0.002432393 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Eliminate Distinct after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Eliminate Distinct has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Finish Analysis after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Finish Analysis has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Inline CTE after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Inline CTE has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Union after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Union has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch OptimizeLimitZero after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch OptimizeLimitZero has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch LocalRelation early after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch LocalRelation early has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Pullup Correlated Expressions after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Pullup Correlated Expressions has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Subquery after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Subquery has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Replace Operators after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Replace Operators has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Aggregate after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Aggregate has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Operator Optimization before Inferring Filters after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Operator Optimization before Inferring Filters has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Infer Filters after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Infer Filters has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Operator Optimization after Inferring Filters after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Operator Optimization after Inferring Filters has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Push extra predicate through join after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Push extra predicate through join has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Clean Up Temporary CTE Info after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Clean Up Temporary CTE Info has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Pre CBO Rules after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Pre CBO Rules has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Early Filter and Projection Push-Down after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Early Filter and Projection Push-Down has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Update CTE Relation Stats after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Update CTE Relation Stats has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Join Reorder after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Join Reorder has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Eliminate Sorts after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Eliminate Sorts has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Decimal Optimizations after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Decimal Optimizations has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Distinct Aggregate Rewrite after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Distinct Aggregate Rewrite has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Object Expressions Optimization after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Object Expressions Optimization has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch LocalRelation after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch LocalRelation has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Optimize One Row Plan after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Optimize One Row Plan has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Check Cartesian Products after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Check Cartesian Products has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch RewriteSubquery after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch RewriteSubquery has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch NormalizeFloatingNumbers after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch NormalizeFloatingNumbers has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch ReplaceUpdateFieldsExpression after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch ReplaceUpdateFieldsExpression has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Optimize Metadata Only Query after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Optimize Metadata Only Query has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch PartitionPruning after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch PartitionPruning has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch InjectRuntimeFilter after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch InjectRuntimeFilter has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch MergeScalarSubqueries after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch MergeScalarSubqueries has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Pushdown Filters from PartitionPruning after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Pushdown Filters from PartitionPruning has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Cleanup filters that cannot be pushed down after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Cleanup filters that cannot be pushed down has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Extract Python UDFs after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Extract Python UDFs has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch User Provided Optimizers after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch User Provided Optimizers has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Replace CTE with Repartition after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Replace CTE with Repartition has no effect.
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 173
Total time: 0.062598303 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Preparations has no effect.
23/11/10 17:21:07 DEBUG SparkSqlParser: Parsing command: select * from (select id + 1 as id from ids) as ids2 left anti join ids on ids2.id = ids.id
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Substitution after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Substitution has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Disable Hints after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Disable Hints has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Hints after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Hints has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Simple Sanity Check after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Simple Sanity Check has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Keep Legacy Outputs after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Keep Legacy Outputs has no effect.
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations ===
 'Project [*]                                       'Project [*]
 +- 'Join LeftAnti, ('ids2.id = 'ids.id)            +- 'Join LeftAnti, ('ids2.id = 'ids.id)
    :- 'SubqueryAlias ids2                             :- 'SubqueryAlias ids2
    :  +- 'Project [('id + 1) AS id#6]                 :  +- 'Project [('id + 1) AS id#6]
!   :     +- 'UnresolvedRelation [ids], [], false      :     +- SubqueryAlias ids
!   +- 'UnresolvedRelation [ids], [], false            :        +- View (`ids`, [id#4])
!                                                      :           +- Deduplicate [id#4]
!                                                      :              +- Project [value#1 AS id#4]
!                                                      :                 +- LocalRelation [value#1]
!                                                      +- SubqueryAlias ids
!                                                         +- View (`ids`, [id#4])
!                                                            +- Deduplicate [id#4]
!                                                               +- Project [value#1 AS id#4]
!                                                                  +- LocalRelation [value#1]
           
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.DeduplicateRelations ===
 'Project [*]                                      'Project [*]
 +- 'Join LeftAnti, ('ids2.id = 'ids.id)           +- 'Join LeftAnti, ('ids2.id = 'ids.id)
    :- 'SubqueryAlias ids2                            :- 'SubqueryAlias ids2
    :  +- 'Project [('id + 1) AS id#6]                :  +- 'Project [('id + 1) AS id#6]
    :     +- SubqueryAlias ids                        :     +- SubqueryAlias ids
    :        +- View (`ids`, [id#4])                  :        +- View (`ids`, [id#4])
    :           +- Deduplicate [id#4]                 :           +- Deduplicate [id#4]
    :              +- Project [value#1 AS id#4]       :              +- Project [value#1 AS id#4]
    :                 +- LocalRelation [value#1]      :                 +- LocalRelation [value#1]
    +- SubqueryAlias ids                              +- SubqueryAlias ids
       +- View (`ids`, [id#4])                           +- View (`ids`, [id#4])
          +- Deduplicate [id#4]                             +- Deduplicate [id#4]
!            +- Project [value#1 AS id#4]                      +- Project [value#7 AS id#4]
!               +- LocalRelation [value#1]                        +- LocalRelation [value#7]
           
23/11/10 17:21:07 TRACE Analyzer$ResolveReferences: Attempting to resolve View (`ids`, [id#4])
23/11/10 17:21:07 TRACE Analyzer$ResolveReferences: Attempting to resolve SubqueryAlias ids
23/11/10 17:21:07 TRACE Analyzer$ResolveReferences: Attempting to resolve 'Project [('id + 1) AS id#6]
23/11/10 17:21:07 DEBUG BaseSessionStateBuilder$$anon$1: Resolving 'id to id#4
23/11/10 17:21:07 TRACE Analyzer$ResolveReferences: Attempting to resolve SubqueryAlias ids2
23/11/10 17:21:07 TRACE Analyzer$ResolveReferences: Attempting to resolve LocalRelation [value#7]
23/11/10 17:21:07 TRACE Analyzer$ResolveReferences: Attempting to resolve Project [value#7 AS id#4]
23/11/10 17:21:07 TRACE Analyzer$ResolveReferences: Attempting to resolve Deduplicate [id#4]
23/11/10 17:21:07 TRACE Analyzer$ResolveReferences: Attempting to resolve View (`ids`, [id#4])
23/11/10 17:21:07 TRACE Analyzer$ResolveReferences: Attempting to resolve SubqueryAlias ids
23/11/10 17:21:07 TRACE Analyzer$ResolveReferences: Attempting to resolve 'Join LeftAnti, ('ids2.id = 'ids.id)
23/11/10 17:21:07 DEBUG BaseSessionStateBuilder$$anon$1: Resolving 'ids2.id to id#6
23/11/10 17:21:07 DEBUG BaseSessionStateBuilder$$anon$1: Resolving 'ids.id to id#4
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences ===
!'Project [*]                                      Project [id#6]
!+- 'Join LeftAnti, ('ids2.id = 'ids.id)           +- Join LeftAnti, (id#6 = id#4)
!   :- 'SubqueryAlias ids2                            :- SubqueryAlias ids2
!   :  +- 'Project [('id + 1) AS id#6]                :  +- Project [(id#4 + 1) AS id#6]
    :     +- SubqueryAlias ids                        :     +- SubqueryAlias ids
    :        +- View (`ids`, [id#4])                  :        +- View (`ids`, [id#4])
    :           +- Deduplicate [id#4]                 :           +- Deduplicate [id#4]
    :              +- Project [value#1 AS id#4]       :              +- Project [value#1 AS id#4]
    :                 +- LocalRelation [value#1]      :                 +- LocalRelation [value#1]
    +- SubqueryAlias ids                              +- SubqueryAlias ids
       +- View (`ids`, [id#4])                           +- View (`ids`, [id#4])
          +- Deduplicate [id#4]                             +- Deduplicate [id#4]
             +- Project [value#7 AS id#4]                      +- Project [value#7 AS id#4]
                +- LocalRelation [value#7]                        +- LocalRelation [value#7]
           
23/11/10 17:21:07 DEBUG DFSClient: WriteChunk allocating new packet seqno=6, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=87552, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:07 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=89375, lastFlushOffset=87578, createNewBlock=false
23/11/10 17:21:07 DEBUG DataStreamer: Queued packet seqno: 6 offsetInBlock: 87552 lastPacketInBlock: false lastByteOffsetInBlock: 89375, blk_1073742676_1857
23/11/10 17:21:07 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 6
23/11/10 17:21:07 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:07 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 6 offsetInBlock: 87552 lastPacketInBlock: false lastByteOffsetInBlock: 89375
23/11/10 17:21:07 DEBUG DataStreamer: DFSClient seqno: 6 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:07 DEBUG DFSClient: WriteChunk allocating new packet seqno=7, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=89088, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:07 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=89487, lastFlushOffset=89375, createNewBlock=false
23/11/10 17:21:07 DEBUG DataStreamer: Queued packet seqno: 7 offsetInBlock: 89088 lastPacketInBlock: false lastByteOffsetInBlock: 89487, blk_1073742676_1857
23/11/10 17:21:07 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 7
23/11/10 17:21:07 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:07 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 7 offsetInBlock: 89088 lastPacketInBlock: false lastByteOffsetInBlock: 89487
23/11/10 17:21:07 DEBUG DataStreamer: DFSClient seqno: 7 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:07 TRACE Analyzer$ResolveReferences: Attempting to resolve Project [(id#4 + 1) AS id#6]
23/11/10 17:21:07 TRACE Analyzer$ResolveReferences: Attempting to resolve SubqueryAlias ids2
23/11/10 17:21:07 TRACE Analyzer$ResolveReferences: Attempting to resolve Join LeftAnti, (id#6 = id#4)
23/11/10 17:21:07 TRACE Analyzer$ResolveReferences: Attempting to resolve Project [id#6]
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Resolution after 2 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Result of Batch Resolution ===
!'Project [*]                                       Project [id#6]
!+- 'Join LeftAnti, ('ids2.id = 'ids.id)            +- Join LeftAnti, (id#6 = id#4)
!   :- 'SubqueryAlias ids2                             :- SubqueryAlias ids2
!   :  +- 'Project [('id + 1) AS id#6]                 :  +- Project [(id#4 + 1) AS id#6]
!   :     +- 'UnresolvedRelation [ids], [], false      :     +- SubqueryAlias ids
!   +- 'UnresolvedRelation [ids], [], false            :        +- View (`ids`, [id#4])
!                                                      :           +- Deduplicate [id#4]
!                                                      :              +- Project [value#1 AS id#4]
!                                                      :                 +- LocalRelation [value#1]
!                                                      +- SubqueryAlias ids
!                                                         +- View (`ids`, [id#4])
!                                                            +- Deduplicate [id#4]
!                                                               +- Project [value#7 AS id#4]
!                                                                  +- LocalRelation [value#7]
          
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove TempResolvedColumn after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Remove TempResolvedColumn has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Apply Char Padding after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Apply Char Padding has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Post-Hoc Resolution after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Post-Hoc Resolution has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove Unresolved Hints after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Remove Unresolved Hints has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Nondeterministic after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Nondeterministic has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UDF after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch UDF has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UpdateNullability after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch UpdateNullability has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Subquery after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Subquery has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Cleanup after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Cleanup has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch HandleAnalysisOnlyCommand after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch HandleAnalysisOnlyCommand has no effect.
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 128
Total time: 0.031711101 seconds
Total number of effective runs: 3
Total time of effective runs: 0.015318555 seconds
      
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Eliminate Distinct after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Eliminate Distinct has no effect.
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis ===
 Project [id#6]                                    Project [id#6]
 +- Join LeftAnti, (id#6 = id#4)                   +- Join LeftAnti, (id#6 = id#4)
!   :- SubqueryAlias ids2                             :- Project [(id#4 + 1) AS id#6]
!   :  +- Project [(id#4 + 1) AS id#6]                :  +- Deduplicate [id#4]
!   :     +- SubqueryAlias ids                        :     +- Project [value#1 AS id#4]
!   :        +- View (`ids`, [id#4])                  :        +- LocalRelation [value#1]
!   :           +- Deduplicate [id#4]                 +- Deduplicate [id#4]
!   :              +- Project [value#1 AS id#4]          +- Project [value#7 AS id#4]
!   :                 +- LocalRelation [value#1]            +- LocalRelation [value#7]
!   +- SubqueryAlias ids                           
!      +- View (`ids`, [id#4])                     
!         +- Deduplicate [id#4]                    
!            +- Project [value#7 AS id#4]          
!               +- LocalRelation [value#7]         
           
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Result of Batch Finish Analysis ===
 Project [id#6]                                    Project [id#6]
 +- Join LeftAnti, (id#6 = id#4)                   +- Join LeftAnti, (id#6 = id#4)
!   :- SubqueryAlias ids2                             :- Project [(id#4 + 1) AS id#6]
!   :  +- Project [(id#4 + 1) AS id#6]                :  +- Deduplicate [id#4]
!   :     +- SubqueryAlias ids                        :     +- Project [value#1 AS id#4]
!   :        +- View (`ids`, [id#4])                  :        +- LocalRelation [value#1]
!   :           +- Deduplicate [id#4]                 +- Deduplicate [id#4]
!   :              +- Project [value#1 AS id#4]          +- Project [value#7 AS id#4]
!   :                 +- LocalRelation [value#1]            +- LocalRelation [value#7]
!   +- SubqueryAlias ids                           
!      +- View (`ids`, [id#4])                     
!         +- Deduplicate [id#4]                    
!            +- Project [value#7 AS id#4]          
!               +- LocalRelation [value#7]         
          
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Inline CTE after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Inline CTE has no effect.
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.RemoveNoopOperators ===
!Project [id#6]                           Join LeftAnti, (id#6 = id#4)
!+- Join LeftAnti, (id#6 = id#4)          :- Project [(id#4 + 1) AS id#6]
!   :- Project [(id#4 + 1) AS id#6]       :  +- Deduplicate [id#4]
!   :  +- Deduplicate [id#4]              :     +- Project [value#1 AS id#4]
!   :     +- Project [value#1 AS id#4]    :        +- LocalRelation [value#1]
!   :        +- LocalRelation [value#1]   +- Deduplicate [id#4]
!   +- Deduplicate [id#4]                    +- Project [value#7 AS id#4]
!      +- Project [value#7 AS id#4]             +- LocalRelation [value#7]
!         +- LocalRelation [value#7]      
           
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Result of Batch Union ===
!Project [id#6]                           Join LeftAnti, (id#6 = id#4)
!+- Join LeftAnti, (id#6 = id#4)          :- Project [(id#4 + 1) AS id#6]
!   :- Project [(id#4 + 1) AS id#6]       :  +- Deduplicate [id#4]
!   :  +- Deduplicate [id#4]              :     +- Project [value#1 AS id#4]
!   :     +- Project [value#1 AS id#4]    :        +- LocalRelation [value#1]
!   :        +- LocalRelation [value#1]   +- Deduplicate [id#4]
!   +- Deduplicate [id#4]                    +- Project [value#7 AS id#4]
!      +- Project [value#7 AS id#4]             +- LocalRelation [value#7]
!         +- LocalRelation [value#7]      
          
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch OptimizeLimitZero after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch OptimizeLimitZero has no effect.
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation ===
 Join LeftAnti, (id#6 = id#4)          Join LeftAnti, (id#6 = id#4)
 :- Project [(id#4 + 1) AS id#6]       :- Project [(id#4 + 1) AS id#6]
 :  +- Deduplicate [id#4]              :  +- Deduplicate [id#4]
!:     +- Project [value#1 AS id#4]    :     +- LocalRelation [id#4]
!:        +- LocalRelation [value#1]   +- Deduplicate [id#4]
!+- Deduplicate [id#4]                    +- LocalRelation [id#4]
!   +- Project [value#7 AS id#4]       
!      +- LocalRelation [value#7]      
           
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch LocalRelation early after 2 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Result of Batch LocalRelation early ===
 Join LeftAnti, (id#6 = id#4)          Join LeftAnti, (id#6 = id#4)
 :- Project [(id#4 + 1) AS id#6]       :- Project [(id#4 + 1) AS id#6]
 :  +- Deduplicate [id#4]              :  +- Deduplicate [id#4]
!:     +- Project [value#1 AS id#4]    :     +- LocalRelation [id#4]
!:        +- LocalRelation [value#1]   +- Deduplicate [id#4]
!+- Deduplicate [id#4]                    +- LocalRelation [id#4]
!   +- Project [value#7 AS id#4]       
!      +- LocalRelation [value#7]      
          
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Pullup Correlated Expressions after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Pullup Correlated Expressions has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Subquery after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Subquery has no effect.
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ReplaceDeduplicateWithAggregate ===
 Join LeftAnti, (id#6 = id#4)      Join LeftAnti, (id#6 = id#4)
 :- Project [(id#4 + 1) AS id#6]   :- Project [(id#4 + 1) AS id#6]
!:  +- Deduplicate [id#4]          :  +- Aggregate [id#4], [id#4]
 :     +- LocalRelation [id#4]     :     +- LocalRelation [id#4]
!+- Deduplicate [id#4]             +- Aggregate [id#4], [id#4]
    +- LocalRelation [id#4]           +- LocalRelation [id#4]
           
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Replace Operators after 2 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Result of Batch Replace Operators ===
 Join LeftAnti, (id#6 = id#4)      Join LeftAnti, (id#6 = id#4)
 :- Project [(id#4 + 1) AS id#6]   :- Project [(id#4 + 1) AS id#6]
!:  +- Deduplicate [id#4]          :  +- Aggregate [id#4], [id#4]
 :     +- LocalRelation [id#4]     :     +- LocalRelation [id#4]
!+- Deduplicate [id#4]             +- Aggregate [id#4], [id#4]
    +- LocalRelation [id#4]           +- LocalRelation [id#4]
          
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Aggregate after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Aggregate has no effect.
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.CollapseProject ===
 Join LeftAnti, (id#6 = id#4)      Join LeftAnti, (id#6 = id#4)
!:- Project [(id#4 + 1) AS id#6]   :- Aggregate [id#4], [(id#4 + 1) AS id#6]
!:  +- Aggregate [id#4], [id#4]    :  +- LocalRelation [id#4]
!:     +- LocalRelation [id#4]     +- Aggregate [id#4], [id#4]
!+- Aggregate [id#4], [id#4]          +- LocalRelation [id#4]
!   +- LocalRelation [id#4]        
           
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushDownLeftSemiAntiJoin ===
!Join LeftAnti, (id#6 = id#4)                'Aggregate [id#4], [(id#4 + 1) AS id#6]
!:- Aggregate [id#4], [(id#4 + 1) AS id#6]   +- 'Join LeftAnti, ((id#4 + 1) = id#4)
!:  +- LocalRelation [id#4]                     :- LocalRelation [id#4]
!+- Aggregate [id#4], [id#4]                    +- Aggregate [id#4], [id#4]
!   +- LocalRelation [id#4]                        +- LocalRelation [id#4]
           
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Operator Optimization before Inferring Filters after 3 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Result of Batch Operator Optimization before Inferring Filters ===
!Join LeftAnti, (id#6 = id#4)      'Aggregate [id#4], [(id#4 + 1) AS id#6]
!:- Project [(id#4 + 1) AS id#6]   +- 'Join LeftAnti, ((id#4 + 1) = id#4)
!:  +- Aggregate [id#4], [id#4]       :- LocalRelation [id#4]
!:     +- LocalRelation [id#4]        +- Aggregate [id#4], [id#4]
!+- Aggregate [id#4], [id#4]             +- LocalRelation [id#4]
!   +- LocalRelation [id#4]        
          
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints ===
 'Aggregate [id#4], [(id#4 + 1) AS id#6]   'Aggregate [id#4], [(id#4 + 1) AS id#6]
 +- 'Join LeftAnti, ((id#4 + 1) = id#4)    +- 'Join LeftAnti, ((id#4 + 1) = id#4)
    :- LocalRelation [id#4]                   :- LocalRelation [id#4]
!   +- Aggregate [id#4], [id#4]               +- Filter ((id#4 + 1) = id#4)
!      +- LocalRelation [id#4]                   +- Aggregate [id#4], [id#4]
!                                                   +- LocalRelation [id#4]
           
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Result of Batch Infer Filters ===
 'Aggregate [id#4], [(id#4 + 1) AS id#6]   'Aggregate [id#4], [(id#4 + 1) AS id#6]
 +- 'Join LeftAnti, ((id#4 + 1) = id#4)    +- 'Join LeftAnti, ((id#4 + 1) = id#4)
    :- LocalRelation [id#4]                   :- LocalRelation [id#4]
!   +- Aggregate [id#4], [id#4]               +- Filter ((id#4 + 1) = id#4)
!      +- LocalRelation [id#4]                   +- Aggregate [id#4], [id#4]
!                                                   +- LocalRelation [id#4]
          
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushDownPredicates ===
 'Aggregate [id#4], [(id#4 + 1) AS id#6]   'Aggregate [id#4], [(id#4 + 1) AS id#6]
 +- 'Join LeftAnti, ((id#4 + 1) = id#4)    +- 'Join LeftAnti, ((id#4 + 1) = id#4)
    :- LocalRelation [id#4]                   :- LocalRelation [id#4]
!   +- Filter ((id#4 + 1) = id#4)             +- Aggregate [id#4], [id#4]
!      +- Aggregate [id#4], [id#4]               +- Filter ((id#4 + 1) = id#4)
          +- LocalRelation [id#4]                   +- LocalRelation [id#4]
           
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Operator Optimization after Inferring Filters after 2 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: 
=== Result of Batch Operator Optimization after Inferring Filters ===
 'Aggregate [id#4], [(id#4 + 1) AS id#6]   'Aggregate [id#4], [(id#4 + 1) AS id#6]
 +- 'Join LeftAnti, ((id#4 + 1) = id#4)    +- 'Join LeftAnti, ((id#4 + 1) = id#4)
    :- LocalRelation [id#4]                   :- LocalRelation [id#4]
!   +- Filter ((id#4 + 1) = id#4)             +- Aggregate [id#4], [id#4]
!      +- Aggregate [id#4], [id#4]               +- Filter ((id#4 + 1) = id#4)
          +- LocalRelation [id#4]                   +- LocalRelation [id#4]
          
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Push extra predicate through join after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Push extra predicate through join has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Clean Up Temporary CTE Info after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Clean Up Temporary CTE Info has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Pre CBO Rules after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Pre CBO Rules has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Early Filter and Projection Push-Down after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Early Filter and Projection Push-Down has no effect.
23/11/10 17:21:07 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Update CTE Relation Stats after 1 iterations.
23/11/10 17:21:07 TRACE PlanChangeLogger: Batch Update CTE Relation Stats has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Join Reorder after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Join Reorder has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Eliminate Sorts after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Eliminate Sorts has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Decimal Optimizations after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Decimal Optimizations has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Distinct Aggregate Rewrite after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Distinct Aggregate Rewrite has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Object Expressions Optimization after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Object Expressions Optimization has no effect.
23/11/10 17:21:08 TRACE package$ExpressionCanonicalizer: Fixed point reached for batch CleanExpressions after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch CleanExpressions has no effect.
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 9.162E-6 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:08 DEBUG GeneratePredicate: Generated predicate '((input[0, int, false] + 1) = input[0, int, false])':
/* 001 */ public SpecificPredicate generate(Object[] references) {
/* 002 */   return new SpecificPredicate(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {
/* 006 */   private final Object[] references;
/* 007 */
/* 008 */
/* 009 */   public SpecificPredicate(Object[] references) {
/* 010 */     this.references = references;
/* 011 */
/* 012 */   }
/* 013 */
/* 014 */   public void initialize(int partitionIndex) {
/* 015 */
/* 016 */   }
/* 017 */
/* 018 */   public boolean eval(InternalRow i) {
/* 019 */
/* 020 */     int value_2 = i.getInt(0);
/* 021 */
/* 022 */     int value_1 = -1;
/* 023 */
/* 024 */     value_1 = value_2 + 1;
/* 025 */     int value_4 = i.getInt(0);
/* 026 */     boolean value_0 = false;
/* 027 */     value_0 = value_1 == value_4;
/* 028 */     return !false && value_0;
/* 029 */   }
/* 030 */
/* 031 */
/* 032 */ }

23/11/10 17:21:08 DEBUG CodeGenerator: 
/* 001 */ public SpecificPredicate generate(Object[] references) {
/* 002 */   return new SpecificPredicate(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificPredicate extends org.apache.spark.sql.catalyst.expressions.BasePredicate {
/* 006 */   private final Object[] references;
/* 007 */
/* 008 */
/* 009 */   public SpecificPredicate(Object[] references) {
/* 010 */     this.references = references;
/* 011 */
/* 012 */   }
/* 013 */
/* 014 */   public void initialize(int partitionIndex) {
/* 015 */
/* 016 */   }
/* 017 */
/* 018 */   public boolean eval(InternalRow i) {
/* 019 */
/* 020 */     int value_2 = i.getInt(0);
/* 021 */
/* 022 */     int value_1 = -1;
/* 023 */
/* 024 */     value_1 = value_2 + 1;
/* 025 */     int value_4 = i.getInt(0);
/* 026 */     boolean value_0 = false;
/* 027 */     value_0 = value_1 == value_4;
/* 028 */     return !false && value_0;
/* 029 */   }
/* 030 */
/* 031 */
/* 032 */ }

23/11/10 17:21:08 INFO CodeGenerator: Code generated in 7.357001 ms
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation ===
 'Aggregate [id#4], [(id#4 + 1) AS id#6]   'Aggregate [id#4], [(id#4 + 1) AS id#6]
 +- 'Join LeftAnti, ((id#4 + 1) = id#4)    +- 'Join LeftAnti, ((id#4 + 1) = id#4)
    :- LocalRelation [id#4]                   :- LocalRelation [id#4]
    +- Aggregate [id#4], [id#4]               +- Aggregate [id#4], [id#4]
!      +- Filter ((id#4 + 1) = id#4)             +- LocalRelation <empty>, [id#4]
!         +- LocalRelation [id#4]          
           
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PropagateEmptyRelation ===
!'Aggregate [id#4], [(id#4 + 1) AS id#6]   Aggregate [id#4], [(id#4 + 1) AS id#6]
!+- 'Join LeftAnti, ((id#4 + 1) = id#4)    +- LocalRelation [id#4]
!   :- LocalRelation [id#4]                
!   +- Aggregate [id#4], [id#4]            
!      +- LocalRelation <empty>, [id#4]    
           
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch LocalRelation after 2 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Result of Batch LocalRelation ===
!'Aggregate [id#4], [(id#4 + 1) AS id#6]   Aggregate [id#4], [(id#4 + 1) AS id#6]
!+- 'Join LeftAnti, ((id#4 + 1) = id#4)    +- LocalRelation [id#4]
!   :- LocalRelation [id#4]                
!   +- Aggregate [id#4], [id#4]            
!      +- Filter ((id#4 + 1) = id#4)       
!         +- LocalRelation [id#4]          
          
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Optimize One Row Plan after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Optimize One Row Plan has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Check Cartesian Products after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Check Cartesian Products has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch RewriteSubquery after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch RewriteSubquery has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch NormalizeFloatingNumbers after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch NormalizeFloatingNumbers has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch ReplaceUpdateFieldsExpression after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch ReplaceUpdateFieldsExpression has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Optimize Metadata Only Query after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Optimize Metadata Only Query has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch PartitionPruning after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch PartitionPruning has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch InjectRuntimeFilter after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch InjectRuntimeFilter has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch MergeScalarSubqueries after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch MergeScalarSubqueries has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Pushdown Filters from PartitionPruning after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Pushdown Filters from PartitionPruning has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Cleanup filters that cannot be pushed down after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Cleanup filters that cannot be pushed down has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Extract Python UDFs after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Extract Python UDFs has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch User Provided Optimizers after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch User Provided Optimizers has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$2: Fixed point reached for batch Replace CTE with Repartition after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Replace CTE with Repartition has no effect.
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 337
Total time: 0.068428484 seconds
Total number of effective runs: 10
Total time of effective runs: 0.041984997 seconds
      
23/11/10 17:21:08 DEBUG InsertAdaptiveSparkPlan: Adaptive execution enabled for plan: HashAggregate(keys=[id#4], functions=[], output=[id#6])
+- HashAggregate(keys=[id#4], functions=[], output=[id#4])
   +- LocalTableScan [id#4]

23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.exchange.EnsureRequirements ===
 HashAggregate(keys=[id#4], functions=[], output=[id#6])      HashAggregate(keys=[id#4], functions=[], output=[id#6])
!+- HashAggregate(keys=[id#4], functions=[], output=[id#4])   +- Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#14]
!   +- LocalTableScan [id#4]                                     +- HashAggregate(keys=[id#4], functions=[], output=[id#4])
!                                                                   +- LocalTableScan [id#4]
           
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Result of Batch AQE Preparations ===
 HashAggregate(keys=[id#4], functions=[], output=[id#6])      HashAggregate(keys=[id#4], functions=[], output=[id#6])
!+- HashAggregate(keys=[id#4], functions=[], output=[id#4])   +- Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#14]
!   +- LocalTableScan [id#4]                                     +- HashAggregate(keys=[id#4], functions=[], output=[id#4])
!                                                                   +- LocalTableScan [id#4]
          
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan ===
!HashAggregate(keys=[id#4], functions=[], output=[id#6])      AdaptiveSparkPlan isFinalPlan=false
!+- HashAggregate(keys=[id#4], functions=[], output=[id#4])   +- HashAggregate(keys=[id#4], functions=[], output=[id#6])
!   +- LocalTableScan [id#4]                                     +- Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#14]
!                                                                   +- HashAggregate(keys=[id#4], functions=[], output=[id#4])
!                                                                      +- LocalTableScan [id#4]
           
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Result of Batch Preparations ===
!HashAggregate(keys=[id#4], functions=[], output=[id#6])      AdaptiveSparkPlan isFinalPlan=false
!+- HashAggregate(keys=[id#4], functions=[], output=[id#4])   +- HashAggregate(keys=[id#4], functions=[], output=[id#6])
!   +- LocalTableScan [id#4]                                     +- Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#14]
!                                                                   +- HashAggregate(keys=[id#4], functions=[], output=[id#4])
!                                                                      +- LocalTableScan [id#4]
          
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Substitution after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Substitution has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Disable Hints after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Disable Hints has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Hints after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Hints has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Simple Sanity Check after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Simple Sanity Check has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Keep Legacy Outputs after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Keep Legacy Outputs has no effect.
23/11/10 17:21:08 DEBUG DFSClient: WriteChunk allocating new packet seqno=8, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=89088, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:08 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=93740, lastFlushOffset=89487, createNewBlock=false
23/11/10 17:21:08 DEBUG DataStreamer: Queued packet seqno: 8 offsetInBlock: 89088 lastPacketInBlock: false lastByteOffsetInBlock: 93740, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 8
23/11/10 17:21:08 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 8 offsetInBlock: 89088 lastPacketInBlock: false lastByteOffsetInBlock: 93740
23/11/10 17:21:08 DEBUG DataStreamer: DFSClient seqno: 8 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:08 TRACE Analyzer$ResolveReferences: Attempting to resolve LocalRelation <empty>, [id#6]
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, StructField(id,IntegerType,false)), StructField(id,IntegerType,false))), obj#9: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(id#6, StructField(id,IntegerType,false)), obj#9: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [id#6]                                                                                                                                                               +- LocalRelation <empty>, [id#6]
           
23/11/10 17:21:08 TRACE Analyzer$ResolveReferences: Attempting to resolve DeserializeToObject createexternalrow(id#6, StructField(id,IntegerType,false)), obj#9: org.apache.spark.sql.Row
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Resolution after 2 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Result of Batch Resolution ===
!'DeserializeToObject unresolveddeserializer(createexternalrow(getcolumnbyordinal(0, StructField(id,IntegerType,false)), StructField(id,IntegerType,false))), obj#9: org.apache.spark.sql.Row   DeserializeToObject createexternalrow(id#6, StructField(id,IntegerType,false)), obj#9: org.apache.spark.sql.Row
 +- LocalRelation <empty>, [id#6]                                                                                                                                                               +- LocalRelation <empty>, [id#6]
          
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove TempResolvedColumn after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Remove TempResolvedColumn has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Apply Char Padding after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Apply Char Padding has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Post-Hoc Resolution after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Post-Hoc Resolution has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Remove Unresolved Hints after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Remove Unresolved Hints has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Nondeterministic after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Nondeterministic has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UDF after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch UDF has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch UpdateNullability after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch UpdateNullability has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Subquery after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Subquery has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch Cleanup after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Cleanup has no effect.
23/11/10 17:21:08 TRACE BaseSessionStateBuilder$$anon$1: Fixed point reached for batch HandleAnalysisOnlyCommand after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch HandleAnalysisOnlyCommand has no effect.
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 128
Total time: 0.00216343 seconds
Total number of effective runs: 1
Total time of effective runs: 3.2554E-4 seconds
      
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch AQE Query Stage Optimization has no effect.
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.CollapseCodegenStages ===
!Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#14]   Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#22]
!+- HashAggregate(keys=[id#4], functions=[], output=[id#4])            +- *(1) HashAggregate(keys=[id#4], functions=[], output=[id#4])
!   +- LocalTableScan [id#4]                                              +- *(1) LocalTableScan [id#4]
           
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Result of Batch AQE Post Stage Creation ===
!Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#14]   Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#22]
!+- HashAggregate(keys=[id#4], functions=[], output=[id#4])            +- *(1) HashAggregate(keys=[id#4], functions=[], output=[id#4])
!   +- LocalTableScan [id#4]                                              +- *(1) LocalTableScan [id#4]
          
23/11/10 17:21:08 DEBUG ShuffleQueryStageExec: Materialize query stage ShuffleQueryStageExec: 0
23/11/10 17:21:08 DEBUG DFSClient: WriteChunk allocating new packet seqno=9, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=93696, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:08 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=97746, lastFlushOffset=93740, createNewBlock=false
23/11/10 17:21:08 DEBUG DataStreamer: Queued packet seqno: 9 offsetInBlock: 93696 lastPacketInBlock: false lastByteOffsetInBlock: 97746, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 9
23/11/10 17:21:08 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 9 offsetInBlock: 93696 lastPacketInBlock: false lastByteOffsetInBlock: 97746
23/11/10 17:21:08 DEBUG DataStreamer: DFSClient seqno: 9 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:08 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
23/11/10 17:21:08 DEBUG WholeStageCodegenExec: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean hashAgg_initAgg_0;
/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;
/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;
/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;
/* 013 */   private scala.collection.Iterator localtablescan_input_0;
/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 015 */
/* 016 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 017 */     this.references = references;
/* 018 */   }
/* 019 */
/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 021 */     partitionIndex = index;
/* 022 */     this.inputs = inputs;
/* 023 */
/* 024 */     localtablescan_input_0 = inputs[0];
/* 025 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 026 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {
/* 031 */     while ( localtablescan_input_0.hasNext()) {
/* 032 */       InternalRow localtablescan_row_0 = (InternalRow) localtablescan_input_0.next();
/* 033 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);
/* 034 */       int localtablescan_value_0 = localtablescan_row_0.getInt(0);
/* 035 */
/* 036 */       hashAgg_doConsume_0(localtablescan_row_0, localtablescan_value_0);
/* 037 */       // shouldStop check is eliminated
/* 038 */     }
/* 039 */
/* 040 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));
/* 041 */   }
/* 042 */
/* 043 */   private void hashAgg_doConsume_0(InternalRow localtablescan_row_0, int hashAgg_expr_0_0) throws java.io.IOException {
/* 044 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;
/* 045 */
/* 046 */     // generate grouping key
/* 047 */     hashAgg_mutableStateArray_0[0].reset();
/* 048 */
/* 049 */     hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);
/* 050 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();
/* 051 */     if (true) {
/* 052 */       // try to get the buffer from hash map
/* 053 */       hashAgg_unsafeRowAggBuffer_0 =
/* 054 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);
/* 055 */     }
/* 056 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 057 */     // aggregation after processing all input rows.
/* 058 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {
/* 059 */       if (hashAgg_sorter_0 == null) {
/* 060 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();
/* 061 */       } else {
/* 062 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());
/* 063 */       }
/* 064 */
/* 065 */       // the hash map had be spilled, it should have enough memory now,
/* 066 */       // try to allocate buffer again.
/* 067 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 068 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);
/* 069 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {
/* 070 */         // failed to allocate the first page
/* 071 */         throw new org.apache.spark.memory.SparkOutOfMemoryError("No enough memory for aggregation");
/* 072 */       }
/* 073 */     }
/* 074 */
/* 075 */     // common sub-expressions
/* 076 */
/* 077 */     // evaluate aggregate functions and update aggregation buffers
/* 078 */
/* 079 */   }
/* 080 */
/* 081 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)
/* 082 */   throws java.io.IOException {
/* 083 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numOutputRows */).add(1);
/* 084 */
/* 085 */     int hashAgg_value_2 = hashAgg_keyTerm_0.getInt(0);
/* 086 */     hashAgg_mutableStateArray_0[1].reset();
/* 087 */
/* 088 */     hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_2);
/* 089 */     append((hashAgg_mutableStateArray_0[1].getRow()));
/* 090 */
/* 091 */   }
/* 092 */
/* 093 */   protected void processNext() throws java.io.IOException {
/* 094 */     if (!hashAgg_initAgg_0) {
/* 095 */       hashAgg_initAgg_0 = true;
/* 096 */
/* 097 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 098 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 099 */       hashAgg_doAggregateWithKeys_0();
/* 100 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 101 */     }
/* 102 */     // output the result
/* 103 */
/* 104 */     while ( hashAgg_mapIter_0.next()) {
/* 105 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();
/* 106 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();
/* 107 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);
/* 108 */       if (shouldStop()) return;
/* 109 */     }
/* 110 */     hashAgg_mapIter_0.close();
/* 111 */     if (hashAgg_sorter_0 == null) {
/* 112 */       hashAgg_hashMap_0.free();
/* 113 */     }
/* 114 */   }
/* 115 */
/* 116 */ }

23/11/10 17:21:08 DEBUG CodeGenerator: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean hashAgg_initAgg_0;
/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;
/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;
/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;
/* 013 */   private scala.collection.Iterator localtablescan_input_0;
/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 015 */
/* 016 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 017 */     this.references = references;
/* 018 */   }
/* 019 */
/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 021 */     partitionIndex = index;
/* 022 */     this.inputs = inputs;
/* 023 */
/* 024 */     localtablescan_input_0 = inputs[0];
/* 025 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 026 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {
/* 031 */     while ( localtablescan_input_0.hasNext()) {
/* 032 */       InternalRow localtablescan_row_0 = (InternalRow) localtablescan_input_0.next();
/* 033 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);
/* 034 */       int localtablescan_value_0 = localtablescan_row_0.getInt(0);
/* 035 */
/* 036 */       hashAgg_doConsume_0(localtablescan_row_0, localtablescan_value_0);
/* 037 */       // shouldStop check is eliminated
/* 038 */     }
/* 039 */
/* 040 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));
/* 041 */   }
/* 042 */
/* 043 */   private void hashAgg_doConsume_0(InternalRow localtablescan_row_0, int hashAgg_expr_0_0) throws java.io.IOException {
/* 044 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;
/* 045 */
/* 046 */     // generate grouping key
/* 047 */     hashAgg_mutableStateArray_0[0].reset();
/* 048 */
/* 049 */     hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);
/* 050 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();
/* 051 */     if (true) {
/* 052 */       // try to get the buffer from hash map
/* 053 */       hashAgg_unsafeRowAggBuffer_0 =
/* 054 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);
/* 055 */     }
/* 056 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 057 */     // aggregation after processing all input rows.
/* 058 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {
/* 059 */       if (hashAgg_sorter_0 == null) {
/* 060 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();
/* 061 */       } else {
/* 062 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());
/* 063 */       }
/* 064 */
/* 065 */       // the hash map had be spilled, it should have enough memory now,
/* 066 */       // try to allocate buffer again.
/* 067 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 068 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);
/* 069 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {
/* 070 */         // failed to allocate the first page
/* 071 */         throw new org.apache.spark.memory.SparkOutOfMemoryError("No enough memory for aggregation");
/* 072 */       }
/* 073 */     }
/* 074 */
/* 075 */     // common sub-expressions
/* 076 */
/* 077 */     // evaluate aggregate functions and update aggregation buffers
/* 078 */
/* 079 */   }
/* 080 */
/* 081 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)
/* 082 */   throws java.io.IOException {
/* 083 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numOutputRows */).add(1);
/* 084 */
/* 085 */     int hashAgg_value_2 = hashAgg_keyTerm_0.getInt(0);
/* 086 */     hashAgg_mutableStateArray_0[1].reset();
/* 087 */
/* 088 */     hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_2);
/* 089 */     append((hashAgg_mutableStateArray_0[1].getRow()));
/* 090 */
/* 091 */   }
/* 092 */
/* 093 */   protected void processNext() throws java.io.IOException {
/* 094 */     if (!hashAgg_initAgg_0) {
/* 095 */       hashAgg_initAgg_0 = true;
/* 096 */
/* 097 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 098 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 099 */       hashAgg_doAggregateWithKeys_0();
/* 100 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[7] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 101 */     }
/* 102 */     // output the result
/* 103 */
/* 104 */     while ( hashAgg_mapIter_0.next()) {
/* 105 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();
/* 106 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();
/* 107 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);
/* 108 */       if (shouldStop()) return;
/* 109 */     }
/* 110 */     hashAgg_mapIter_0.close();
/* 111 */     if (hashAgg_sorter_0 == null) {
/* 112 */       hashAgg_hashMap_0.free();
/* 113 */     }
/* 114 */   }
/* 115 */
/* 116 */ }

23/11/10 17:21:08 INFO CodeGenerator: Code generated in 24.606501 ms
23/11/10 17:21:08 TRACE package$ExpressionCanonicalizer: Fixed point reached for batch CleanExpressions after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch CleanExpressions has no effect.
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 4.257E-6 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:08 DEBUG GenerateUnsafeProjection: code for input[0, int, false]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */
/* 030 */
/* 031 */     int value_0 = i.getInt(0);
/* 032 */     mutableStateArray_0[0].write(0, value_0);
/* 033 */     return (mutableStateArray_0[0].getRow());
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */ }

23/11/10 17:21:08 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted
23/11/10 17:21:08 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
23/11/10 17:21:08 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 2 took 0.000573 seconds
23/11/10 17:21:08 DEBUG DAGScheduler: Merging stage rdd profiles: Set()
23/11/10 17:21:08 DEBUG DFSClient: WriteChunk allocating new packet seqno=10, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=97280, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:08 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=97866, lastFlushOffset=97746, createNewBlock=false
23/11/10 17:21:08 DEBUG DataStreamer: Queued packet seqno: 10 offsetInBlock: 97280 lastPacketInBlock: false lastByteOffsetInBlock: 97866, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 10
23/11/10 17:21:08 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 10 offsetInBlock: 97280 lastPacketInBlock: false lastByteOffsetInBlock: 97866
23/11/10 17:21:08 DEBUG DataStreamer: DFSClient seqno: 10 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:08 INFO DAGScheduler: Registering RDD 2 (collect at Bug1.scala:18) as input to shuffle 0
23/11/10 17:21:08 INFO DAGScheduler: Got map stage job 0 (collect at Bug1.scala:18) with 3 output partitions
23/11/10 17:21:08 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (collect at Bug1.scala:18)
23/11/10 17:21:08 INFO DAGScheduler: Parents of final stage: List()
23/11/10 17:21:08 INFO DAGScheduler: Missing parents: List()
23/11/10 17:21:08 DEBUG DAGScheduler: submitStage(ShuffleMapStage 0 (name=collect at Bug1.scala:18;jobs=0))
23/11/10 17:21:08 DEBUG DAGScheduler: missing: List()
23/11/10 17:21:08 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at collect at Bug1.scala:18), which has no missing parents
23/11/10 17:21:08 DEBUG DAGScheduler: submitMissingTasks(ShuffleMapStage 0)
23/11/10 17:21:08 DEBUG DFSClient: WriteChunk allocating new packet seqno=11, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=97792, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:08 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=102691, lastFlushOffset=97866, createNewBlock=false
23/11/10 17:21:08 DEBUG DataStreamer: Queued packet seqno: 11 offsetInBlock: 97792 lastPacketInBlock: false lastByteOffsetInBlock: 102691, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 11
23/11/10 17:21:08 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 11 offsetInBlock: 97792 lastPacketInBlock: false lastByteOffsetInBlock: 102691
23/11/10 17:21:08 DEBUG DataStreamer: DFSClient seqno: 11 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 trying to put broadcast_0
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 trying to acquire write lock for broadcast_0
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 acquired write lock for broadcast_0
23/11/10 17:21:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 16.7 KiB, free 366.3 MiB)
23/11/10 17:21:08 DEBUG BlockManager: Put block broadcast_0 locally took 16 ms
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 releasing lock for broadcast_0
23/11/10 17:21:08 DEBUG BlockManager: Putting block broadcast_0 without replication took 17 ms
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 trying to put broadcast_0_piece0
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 trying to acquire write lock for broadcast_0_piece0
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 acquired write lock for broadcast_0_piece0
23/11/10 17:21:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 366.3 MiB)
23/11/10 17:21:08 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, Jiahao, 37589, None)
23/11/10 17:21:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on Jiahao:37589 (size: 8.1 KiB, free: 366.3 MiB)
23/11/10 17:21:08 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0
23/11/10 17:21:08 DEBUG BlockManager: Told master about block broadcast_0_piece0
23/11/10 17:21:08 DEBUG BlockManager: Put block broadcast_0_piece0 locally took 4 ms
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 releasing lock for broadcast_0_piece0
23/11/10 17:21:08 DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took 4 ms
23/11/10 17:21:08 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513
23/11/10 17:21:08 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at collect at Bug1.scala:18) (first 15 tasks are for partitions Vector(0, 1, 2))
23/11/10 17:21:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 3 tasks resource profile 0
23/11/10 17:21:08 DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0
23/11/10 17:21:08 DEBUG TaskSetManager: Adding pending tasks took 1 ms
23/11/10 17:21:08 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY
23/11/10 17:21:08 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0
23/11/10 17:21:08 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY
23/11/10 17:21:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (Jiahao, executor driver, partition 0, PROCESS_LOCAL, 4614 bytes) taskResourceAssignments Map()
23/11/10 17:21:08 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (Jiahao, executor driver, partition 1, PROCESS_LOCAL, 4614 bytes) taskResourceAssignments Map()
23/11/10 17:21:08 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (Jiahao, executor driver, partition 2, PROCESS_LOCAL, 4614 bytes) taskResourceAssignments Map()
23/11/10 17:21:08 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY
23/11/10 17:21:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/10 17:21:08 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
23/11/10 17:21:08 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
23/11/10 17:21:08 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 1
23/11/10 17:21:08 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 2
23/11/10 17:21:08 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 3
23/11/10 17:21:08 DEBUG BlockManager: Getting local block broadcast_0
23/11/10 17:21:08 TRACE BlockInfoManager: Task 2 trying to acquire read lock for broadcast_0
23/11/10 17:21:08 TRACE BlockInfoManager: Task 2 acquired read lock for broadcast_0
23/11/10 17:21:08 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
23/11/10 17:21:08 TRACE package$ExpressionCanonicalizer: Fixed point reached for batch CleanExpressions after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch CleanExpressions has no effect.
23/11/10 17:21:08 TRACE package$ExpressionCanonicalizer: Fixed point reached for batch CleanExpressions after 1 iterations.
23/11/10 17:21:08 TRACE package$ExpressionCanonicalizer: Fixed point reached for batch CleanExpressions after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 3
Total time: 4.8645E-5 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch CleanExpressions has no effect.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch CleanExpressions has no effect.
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 3
Total time: 4.8645E-5 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 3
Total time: 4.8645E-5 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:08 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, int, false], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = false;
/* 032 */     int value_0 = -1;
/* 033 */     if (200 == 0) {
/* 034 */       isNull_0 = true;
/* 035 */     } else {
/* 036 */       int value_1 = 42;
/* 037 */       int value_2 = i.getInt(0);
/* 038 */       value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_2, value_1);
/* 039 */
/* 040 */       int remainder_0 = value_1 % 200;
/* 041 */       if (remainder_0 < 0) {
/* 042 */         value_0=(remainder_0 + 200) % 200;
/* 043 */       } else {
/* 044 */         value_0=remainder_0;
/* 045 */       }
/* 046 */
/* 047 */     }
/* 048 */     if (isNull_0) {
/* 049 */       mutableStateArray_0[0].setNullAt(0);
/* 050 */     } else {
/* 051 */       mutableStateArray_0[0].write(0, value_0);
/* 052 */     }
/* 053 */     return (mutableStateArray_0[0].getRow());
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

23/11/10 17:21:08 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, int, false], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = false;
/* 032 */     int value_0 = -1;
/* 033 */     if (200 == 0) {
/* 034 */       isNull_0 = true;
/* 035 */     } else {
/* 036 */       int value_1 = 42;
/* 037 */       int value_2 = i.getInt(0);
/* 038 */       value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_2, value_1);
/* 039 */
/* 040 */       int remainder_0 = value_1 % 200;
/* 041 */       if (remainder_0 < 0) {
/* 042 */         value_0=(remainder_0 + 200) % 200;
/* 043 */       } else {
/* 044 */         value_0=remainder_0;
/* 045 */       }
/* 046 */
/* 047 */     }
/* 048 */     if (isNull_0) {
/* 049 */       mutableStateArray_0[0].setNullAt(0);
/* 050 */     } else {
/* 051 */       mutableStateArray_0[0].write(0, value_0);
/* 052 */     }
/* 053 */     return (mutableStateArray_0[0].getRow());
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

23/11/10 17:21:08 DEBUG GenerateUnsafeProjection: code for pmod(hash(input[0, int, false], 42), 200):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = false;
/* 032 */     int value_0 = -1;
/* 033 */     if (200 == 0) {
/* 034 */       isNull_0 = true;
/* 035 */     } else {
/* 036 */       int value_1 = 42;
/* 037 */       int value_2 = i.getInt(0);
/* 038 */       value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_2, value_1);
/* 039 */
/* 040 */       int remainder_0 = value_1 % 200;
/* 041 */       if (remainder_0 < 0) {
/* 042 */         value_0=(remainder_0 + 200) % 200;
/* 043 */       } else {
/* 044 */         value_0=remainder_0;
/* 045 */       }
/* 046 */
/* 047 */     }
/* 048 */     if (isNull_0) {
/* 049 */       mutableStateArray_0[0].setNullAt(0);
/* 050 */     } else {
/* 051 */       mutableStateArray_0[0].write(0, value_0);
/* 052 */     }
/* 053 */     return (mutableStateArray_0[0].getRow());
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

23/11/10 17:21:08 DEBUG CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = false;
/* 032 */     int value_0 = -1;
/* 033 */     if (200 == 0) {
/* 034 */       isNull_0 = true;
/* 035 */     } else {
/* 036 */       int value_1 = 42;
/* 037 */       int value_2 = i.getInt(0);
/* 038 */       value_1 = org.apache.spark.unsafe.hash.Murmur3_x86_32.hashInt(value_2, value_1);
/* 039 */
/* 040 */       int remainder_0 = value_1 % 200;
/* 041 */       if (remainder_0 < 0) {
/* 042 */         value_0=(remainder_0 + 200) % 200;
/* 043 */       } else {
/* 044 */         value_0=remainder_0;
/* 045 */       }
/* 046 */
/* 047 */     }
/* 048 */     if (isNull_0) {
/* 049 */       mutableStateArray_0[0].setNullAt(0);
/* 050 */     } else {
/* 051 */       mutableStateArray_0[0].write(0, value_0);
/* 052 */     }
/* 053 */     return (mutableStateArray_0[0].getRow());
/* 054 */   }
/* 055 */
/* 056 */
/* 057 */ }

23/11/10 17:21:08 INFO CodeGenerator: Code generated in 8.605403 ms
23/11/10 17:21:08 DEBUG GenerateUnsafeProjection: code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

23/11/10 17:21:08 DEBUG GenerateUnsafeProjection: code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

23/11/10 17:21:08 DEBUG GenerateUnsafeProjection: code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

23/11/10 17:21:08 DEBUG CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

23/11/10 17:21:08 INFO CodeGenerator: Code generated in 4.835373 ms
23/11/10 17:21:08 TRACE package$ExpressionCanonicalizer: Fixed point reached for batch CleanExpressions after 1 iterations.
23/11/10 17:21:08 TRACE package$ExpressionCanonicalizer: Fixed point reached for batch CleanExpressions after 1 iterations.
23/11/10 17:21:08 TRACE package$ExpressionCanonicalizer: Fixed point reached for batch CleanExpressions after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch CleanExpressions has no effect.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch CleanExpressions has no effect.
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 3
Total time: 1.3513E-5 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 3
Total time: 1.3513E-5 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch CleanExpressions has no effect.
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 3
Total time: 1.3513E-5 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:08 DEBUG GenerateUnsafeProjection: code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

23/11/10 17:21:08 DEBUG GenerateUnsafeProjection: code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

23/11/10 17:21:08 DEBUG GenerateUnsafeProjection: code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

23/11/10 17:21:08 DEBUG CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

23/11/10 17:21:08 INFO CodeGenerator: Code generated in 6.751084 ms
23/11/10 17:21:08 DEBUG TaskMemoryManager: Task 0 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@6e7efba1
23/11/10 17:21:08 DEBUG TaskMemoryManager: Task 2 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@48c7897e
23/11/10 17:21:08 DEBUG TaskMemoryManager: Task 1 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@2423689c
23/11/10 17:21:08 TRACE TaskMemoryManager: Allocate page number 0 (262144 bytes)
23/11/10 17:21:08 TRACE TaskMemoryManager: Allocate page number 0 (262144 bytes)
23/11/10 17:21:08 TRACE TaskMemoryManager: Allocate page number 0 (262144 bytes)
23/11/10 17:21:08 DEBUG GenerateUnsafeProjection: code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

23/11/10 17:21:08 DEBUG GenerateUnsafeProjection: code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

23/11/10 17:21:08 DEBUG GenerateUnsafeProjection: code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

23/11/10 17:21:08 DEBUG TaskMemoryManager: Task 0 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@6e7efba1
23/11/10 17:21:08 DEBUG TaskMemoryManager: Task 1 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@2423689c
23/11/10 17:21:08 DEBUG TaskMemoryManager: Task 2 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@48c7897e
23/11/10 17:21:08 TRACE TaskMemoryManager: Allocate page number 1 (1048576 bytes)
23/11/10 17:21:08 TRACE TaskMemoryManager: Allocate page number 1 (1048576 bytes)
23/11/10 17:21:08 TRACE TaskMemoryManager: Allocate page number 1 (1048576 bytes)
23/11/10 17:21:08 TRACE TaskMemoryManager: Freed page number 0 (262144 bytes)
23/11/10 17:21:08 TRACE TaskMemoryManager: Freed page number 0 (262144 bytes)
23/11/10 17:21:08 TRACE TaskMemoryManager: Freed page number 0 (262144 bytes)
23/11/10 17:21:08 DEBUG TaskMemoryManager: Task 2 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@48c7897e
23/11/10 17:21:08 DEBUG TaskMemoryManager: Task 1 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@2423689c
23/11/10 17:21:08 DEBUG TaskMemoryManager: Task 0 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@6e7efba1
23/11/10 17:21:08 TRACE TaskMemoryManager: Freed page number 1 (1048576 bytes)
23/11/10 17:21:08 TRACE TaskMemoryManager: Freed page number 1 (1048576 bytes)
23/11/10 17:21:08 TRACE TaskMemoryManager: Freed page number 1 (1048576 bytes)
23/11/10 17:21:08 DEBUG TaskMemoryManager: Task 2 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@48c7897e
23/11/10 17:21:08 DEBUG TaskMemoryManager: Task 1 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@2423689c
23/11/10 17:21:08 DEBUG TaskMemoryManager: Task 0 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@6e7efba1
23/11/10 17:21:08 DEBUG LocalDiskShuffleMapOutputWriter: Writing shuffle index file for mapId 1 with length 200
23/11/10 17:21:08 DEBUG LocalDiskShuffleMapOutputWriter: Writing shuffle index file for mapId 2 with length 200
23/11/10 17:21:08 DEBUG LocalDiskShuffleMapOutputWriter: Writing shuffle index file for mapId 0 with length 200
23/11/10 17:21:08 DEBUG IndexShuffleBlockResolver: Shuffle index for mapId 2: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,59,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
23/11/10 17:21:08 DEBUG IndexShuffleBlockResolver: Shuffle index for mapId 0: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,59,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
23/11/10 17:21:08 DEBUG IndexShuffleBlockResolver: Shuffle index for mapId 1: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,59,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
23/11/10 17:21:08 TRACE BlockInfoManager: Task 2 releasing lock for broadcast_0
23/11/10 17:21:08 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2447 bytes result sent to driver
23/11/10 17:21:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2447 bytes result sent to driver
23/11/10 17:21:08 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2447 bytes result sent to driver
23/11/10 17:21:08 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 2
23/11/10 17:21:08 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 1
23/11/10 17:21:08 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 0
23/11/10 17:21:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 237 ms on Jiahao (executor driver) (1/3)
23/11/10 17:21:08 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 226 ms on Jiahao (executor driver) (2/3)
23/11/10 17:21:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 227 ms on Jiahao (executor driver) (3/3)
23/11/10 17:21:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/10 17:21:08 DEBUG DAGScheduler: ShuffleMapTask finished on driver
23/11/10 17:21:08 DEBUG DAGScheduler: ShuffleMapTask finished on driver
23/11/10 17:21:08 DEBUG DAGScheduler: ShuffleMapTask finished on driver
23/11/10 17:21:08 INFO DAGScheduler: ShuffleMapStage 0 (collect at Bug1.scala:18) finished in 0.356 s
23/11/10 17:21:08 INFO DAGScheduler: looking for newly runnable stages
23/11/10 17:21:08 INFO DAGScheduler: running: Set()
23/11/10 17:21:08 INFO DAGScheduler: waiting: Set()
23/11/10 17:21:08 INFO DAGScheduler: failed: Set()
23/11/10 17:21:08 DEBUG MapOutputTrackerMaster: Increasing epoch to 1
23/11/10 17:21:08 DEBUG DAGScheduler: After removal of stage 0, remaining stages = 0
23/11/10 17:21:08 TRACE DAGScheduler: Checking if any dependencies of ShuffleMapStage 0 are now runnable
23/11/10 17:21:08 TRACE DAGScheduler: running: Set()
23/11/10 17:21:08 TRACE DAGScheduler: waiting: Set()
23/11/10 17:21:08 TRACE DAGScheduler: failed: Set()
23/11/10 17:21:08 DEBUG DFSClient: WriteChunk allocating new packet seqno=12, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=102400, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:08 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=125751, lastFlushOffset=102691, createNewBlock=false
23/11/10 17:21:08 DEBUG DataStreamer: Queued packet seqno: 12 offsetInBlock: 102400 lastPacketInBlock: false lastByteOffsetInBlock: 125751, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 12
23/11/10 17:21:08 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 12 offsetInBlock: 102400 lastPacketInBlock: false lastByteOffsetInBlock: 125751
23/11/10 17:21:08 DEBUG DataStreamer: DFSClient seqno: 12 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:08 DEBUG DFSClient: WriteChunk allocating new packet seqno=13, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=125440, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:08 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=125865, lastFlushOffset=125751, createNewBlock=false
23/11/10 17:21:08 DEBUG DataStreamer: Queued packet seqno: 13 offsetInBlock: 125440 lastPacketInBlock: false lastByteOffsetInBlock: 125865, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 13
23/11/10 17:21:08 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 13 offsetInBlock: 125440 lastPacketInBlock: false lastByteOffsetInBlock: 125865
23/11/10 17:21:08 DEBUG DataStreamer: DFSClient seqno: 13 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:08 TRACE AQEOptimizer: Fixed point reached for batch Propagate Empty Relations after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Propagate Empty Relations has no effect.
23/11/10 17:21:08 TRACE AQEOptimizer: Fixed point reached for batch Dynamic Join Selection after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Dynamic Join Selection has no effect.
23/11/10 17:21:08 TRACE AQEOptimizer: Fixed point reached for batch Eliminate Limits after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Eliminate Limits has no effect.
23/11/10 17:21:08 TRACE AQEOptimizer: Fixed point reached for batch Optimize One Row Plan after 1 iterations.
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch Optimize One Row Plan has no effect.
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 6
Total time: 0.001904128 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:08 TRACE PlanChangeLogger: Batch AQE Replanning has no effect.
23/11/10 17:21:08 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions ===
 HashAggregate(keys=[id#4], functions=[], output=[id#6])                     HashAggregate(keys=[id#4], functions=[], output=[id#6])
!+- ShuffleQueryStage 0                                                      +- AQEShuffleRead coalesced
!   +- Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#22]      +- ShuffleQueryStage 0
!      +- *(1) HashAggregate(keys=[id#4], functions=[], output=[id#4])             +- Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#22]
!         +- *(1) LocalTableScan [id#4]                                               +- *(1) HashAggregate(keys=[id#4], functions=[], output=[id#4])
!                                                                                        +- *(1) LocalTableScan [id#4]
           
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Result of Batch AQE Query Stage Optimization ===
 HashAggregate(keys=[id#4], functions=[], output=[id#6])                     HashAggregate(keys=[id#4], functions=[], output=[id#6])
!+- ShuffleQueryStage 0                                                      +- AQEShuffleRead coalesced
!   +- Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#22]      +- ShuffleQueryStage 0
!      +- *(1) HashAggregate(keys=[id#4], functions=[], output=[id#4])             +- Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#22]
!         +- *(1) LocalTableScan [id#4]                                               +- *(1) HashAggregate(keys=[id#4], functions=[], output=[id#4])
!                                                                                        +- *(1) LocalTableScan [id#4]
          
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.CollapseCodegenStages ===
!HashAggregate(keys=[id#4], functions=[], output=[id#6])                        *(2) HashAggregate(keys=[id#4], functions=[], output=[id#6])
 +- AQEShuffleRead coalesced                                                    +- AQEShuffleRead coalesced
    +- ShuffleQueryStage 0                                                         +- ShuffleQueryStage 0
       +- Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#22]         +- Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#22]
          +- *(1) HashAggregate(keys=[id#4], functions=[], output=[id#4])                +- *(1) HashAggregate(keys=[id#4], functions=[], output=[id#4])
             +- *(1) LocalTableScan [id#4]                                                  +- *(1) LocalTableScan [id#4]
           
23/11/10 17:21:08 TRACE PlanChangeLogger: 
=== Result of Batch AQE Post Stage Creation ===
!HashAggregate(keys=[id#4], functions=[], output=[id#6])                        *(2) HashAggregate(keys=[id#4], functions=[], output=[id#6])
 +- AQEShuffleRead coalesced                                                    +- AQEShuffleRead coalesced
    +- ShuffleQueryStage 0                                                         +- ShuffleQueryStage 0
       +- Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#22]         +- Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#22]
          +- *(1) HashAggregate(keys=[id#4], functions=[], output=[id#4])                +- *(1) HashAggregate(keys=[id#4], functions=[], output=[id#4])
             +- *(1) LocalTableScan [id#4]                                                  +- *(1) LocalTableScan [id#4]
          
23/11/10 17:21:08 DEBUG DFSClient: WriteChunk allocating new packet seqno=14, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=125440, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:08 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=130636, lastFlushOffset=125865, createNewBlock=false
23/11/10 17:21:08 DEBUG DataStreamer: Queued packet seqno: 14 offsetInBlock: 125440 lastPacketInBlock: false lastByteOffsetInBlock: 130636, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 14
23/11/10 17:21:08 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
23/11/10 17:21:08 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 14 offsetInBlock: 125440 lastPacketInBlock: false lastByteOffsetInBlock: 130636
23/11/10 17:21:08 DEBUG DataStreamer: DFSClient seqno: 14 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:08 DEBUG WholeStageCodegenExec: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean hashAgg_initAgg_0;
/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;
/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;
/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;
/* 013 */   private scala.collection.Iterator inputadapter_input_0;
/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 015 */
/* 016 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 017 */     this.references = references;
/* 018 */   }
/* 019 */
/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 021 */     partitionIndex = index;
/* 022 */     this.inputs = inputs;
/* 023 */
/* 024 */     inputadapter_input_0 = inputs[0];
/* 025 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 026 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {
/* 031 */     while ( inputadapter_input_0.hasNext()) {
/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 033 */
/* 034 */       int inputadapter_value_0 = inputadapter_row_0.getInt(0);
/* 035 */
/* 036 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0);
/* 037 */       // shouldStop check is eliminated
/* 038 */     }
/* 039 */
/* 040 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));
/* 041 */   }
/* 042 */
/* 043 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, int hashAgg_expr_0_0) throws java.io.IOException {
/* 044 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;
/* 045 */
/* 046 */     // generate grouping key
/* 047 */     hashAgg_mutableStateArray_0[0].reset();
/* 048 */
/* 049 */     hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);
/* 050 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();
/* 051 */     if (true) {
/* 052 */       // try to get the buffer from hash map
/* 053 */       hashAgg_unsafeRowAggBuffer_0 =
/* 054 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);
/* 055 */     }
/* 056 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 057 */     // aggregation after processing all input rows.
/* 058 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {
/* 059 */       if (hashAgg_sorter_0 == null) {
/* 060 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();
/* 061 */       } else {
/* 062 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());
/* 063 */       }
/* 064 */
/* 065 */       // the hash map had be spilled, it should have enough memory now,
/* 066 */       // try to allocate buffer again.
/* 067 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 068 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);
/* 069 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {
/* 070 */         // failed to allocate the first page
/* 071 */         throw new org.apache.spark.memory.SparkOutOfMemoryError("No enough memory for aggregation");
/* 072 */       }
/* 073 */     }
/* 074 */
/* 075 */     // common sub-expressions
/* 076 */
/* 077 */     // evaluate aggregate functions and update aggregation buffers
/* 078 */
/* 079 */   }
/* 080 */
/* 081 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)
/* 082 */   throws java.io.IOException {
/* 083 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);
/* 084 */
/* 085 */     int hashAgg_value_3 = hashAgg_keyTerm_0.getInt(0);
/* 086 */
/* 087 */     int hashAgg_value_2 = -1;
/* 088 */
/* 089 */     hashAgg_value_2 = hashAgg_value_3 + 1;
/* 090 */     hashAgg_mutableStateArray_0[1].reset();
/* 091 */
/* 092 */     hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_2);
/* 093 */     append((hashAgg_mutableStateArray_0[1].getRow()));
/* 094 */
/* 095 */   }
/* 096 */
/* 097 */   protected void processNext() throws java.io.IOException {
/* 098 */     if (!hashAgg_initAgg_0) {
/* 099 */       hashAgg_initAgg_0 = true;
/* 100 */
/* 101 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 102 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 103 */       hashAgg_doAggregateWithKeys_0();
/* 104 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 105 */     }
/* 106 */     // output the result
/* 107 */
/* 108 */     while ( hashAgg_mapIter_0.next()) {
/* 109 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();
/* 110 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();
/* 111 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);
/* 112 */       if (shouldStop()) return;
/* 113 */     }
/* 114 */     hashAgg_mapIter_0.close();
/* 115 */     if (hashAgg_sorter_0 == null) {
/* 116 */       hashAgg_hashMap_0.free();
/* 117 */     }
/* 118 */   }
/* 119 */
/* 120 */ }

23/11/10 17:21:08 DEBUG CodeGenerator: 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=2
/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private boolean hashAgg_initAgg_0;
/* 010 */   private org.apache.spark.unsafe.KVIterator hashAgg_mapIter_0;
/* 011 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap hashAgg_hashMap_0;
/* 012 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter hashAgg_sorter_0;
/* 013 */   private scala.collection.Iterator inputadapter_input_0;
/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] hashAgg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 015 */
/* 016 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 017 */     this.references = references;
/* 018 */   }
/* 019 */
/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 021 */     partitionIndex = index;
/* 022 */     this.inputs = inputs;
/* 023 */
/* 024 */     inputadapter_input_0 = inputs[0];
/* 025 */     hashAgg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 026 */     hashAgg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 027 */
/* 028 */   }
/* 029 */
/* 030 */   private void hashAgg_doAggregateWithKeys_0() throws java.io.IOException {
/* 031 */     while ( inputadapter_input_0.hasNext()) {
/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 033 */
/* 034 */       int inputadapter_value_0 = inputadapter_row_0.getInt(0);
/* 035 */
/* 036 */       hashAgg_doConsume_0(inputadapter_row_0, inputadapter_value_0);
/* 037 */       // shouldStop check is eliminated
/* 038 */     }
/* 039 */
/* 040 */     hashAgg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(hashAgg_hashMap_0, hashAgg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numTasksFallBacked */));
/* 041 */   }
/* 042 */
/* 043 */   private void hashAgg_doConsume_0(InternalRow inputadapter_row_0, int hashAgg_expr_0_0) throws java.io.IOException {
/* 044 */     UnsafeRow hashAgg_unsafeRowAggBuffer_0 = null;
/* 045 */
/* 046 */     // generate grouping key
/* 047 */     hashAgg_mutableStateArray_0[0].reset();
/* 048 */
/* 049 */     hashAgg_mutableStateArray_0[0].write(0, hashAgg_expr_0_0);
/* 050 */     int hashAgg_unsafeRowKeyHash_0 = (hashAgg_mutableStateArray_0[0].getRow()).hashCode();
/* 051 */     if (true) {
/* 052 */       // try to get the buffer from hash map
/* 053 */       hashAgg_unsafeRowAggBuffer_0 =
/* 054 */       hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow((hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);
/* 055 */     }
/* 056 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 057 */     // aggregation after processing all input rows.
/* 058 */     if (hashAgg_unsafeRowAggBuffer_0 == null) {
/* 059 */       if (hashAgg_sorter_0 == null) {
/* 060 */         hashAgg_sorter_0 = hashAgg_hashMap_0.destructAndCreateExternalSorter();
/* 061 */       } else {
/* 062 */         hashAgg_sorter_0.merge(hashAgg_hashMap_0.destructAndCreateExternalSorter());
/* 063 */       }
/* 064 */
/* 065 */       // the hash map had be spilled, it should have enough memory now,
/* 066 */       // try to allocate buffer again.
/* 067 */       hashAgg_unsafeRowAggBuffer_0 = hashAgg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 068 */         (hashAgg_mutableStateArray_0[0].getRow()), hashAgg_unsafeRowKeyHash_0);
/* 069 */       if (hashAgg_unsafeRowAggBuffer_0 == null) {
/* 070 */         // failed to allocate the first page
/* 071 */         throw new org.apache.spark.memory.SparkOutOfMemoryError("No enough memory for aggregation");
/* 072 */       }
/* 073 */     }
/* 074 */
/* 075 */     // common sub-expressions
/* 076 */
/* 077 */     // evaluate aggregate functions and update aggregation buffers
/* 078 */
/* 079 */   }
/* 080 */
/* 081 */   private void hashAgg_doAggregateWithKeysOutput_0(UnsafeRow hashAgg_keyTerm_0, UnsafeRow hashAgg_bufferTerm_0)
/* 082 */   throws java.io.IOException {
/* 083 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* numOutputRows */).add(1);
/* 084 */
/* 085 */     int hashAgg_value_3 = hashAgg_keyTerm_0.getInt(0);
/* 086 */
/* 087 */     int hashAgg_value_2 = -1;
/* 088 */
/* 089 */     hashAgg_value_2 = hashAgg_value_3 + 1;
/* 090 */     hashAgg_mutableStateArray_0[1].reset();
/* 091 */
/* 092 */     hashAgg_mutableStateArray_0[1].write(0, hashAgg_value_2);
/* 093 */     append((hashAgg_mutableStateArray_0[1].getRow()));
/* 094 */
/* 095 */   }
/* 096 */
/* 097 */   protected void processNext() throws java.io.IOException {
/* 098 */     if (!hashAgg_initAgg_0) {
/* 099 */       hashAgg_initAgg_0 = true;
/* 100 */
/* 101 */       hashAgg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 102 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 103 */       hashAgg_doAggregateWithKeys_0();
/* 104 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 105 */     }
/* 106 */     // output the result
/* 107 */
/* 108 */     while ( hashAgg_mapIter_0.next()) {
/* 109 */       UnsafeRow hashAgg_aggKey_0 = (UnsafeRow) hashAgg_mapIter_0.getKey();
/* 110 */       UnsafeRow hashAgg_aggBuffer_0 = (UnsafeRow) hashAgg_mapIter_0.getValue();
/* 111 */       hashAgg_doAggregateWithKeysOutput_0(hashAgg_aggKey_0, hashAgg_aggBuffer_0);
/* 112 */       if (shouldStop()) return;
/* 113 */     }
/* 114 */     hashAgg_mapIter_0.close();
/* 115 */     if (hashAgg_sorter_0 == null) {
/* 116 */       hashAgg_hashMap_0.free();
/* 117 */     }
/* 118 */   }
/* 119 */
/* 120 */ }

23/11/10 17:21:08 INFO CodeGenerator: Code generated in 9.776073 ms
23/11/10 17:21:08 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted
23/11/10 17:21:08 DEBUG DFSClient: WriteChunk allocating new packet seqno=15, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=130560, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:08 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=130770, lastFlushOffset=130636, createNewBlock=false
23/11/10 17:21:08 DEBUG DataStreamer: Queued packet seqno: 15 offsetInBlock: 130560 lastPacketInBlock: false lastByteOffsetInBlock: 130770, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 15
23/11/10 17:21:08 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 15 offsetInBlock: 130560 lastPacketInBlock: false lastByteOffsetInBlock: 130770
23/11/10 17:21:08 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
23/11/10 17:21:08 DEBUG DataStreamer: DFSClient seqno: 15 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:08 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2
23/11/10 17:21:08 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++
23/11/10 17:21:08 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5
23/11/10 17:21:08 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
23/11/10 17:21:08 INFO SparkContext: Starting job: collect at Bug1.scala:18
23/11/10 17:21:08 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 5 took 0.000119 seconds
23/11/10 17:21:08 DEBUG DAGScheduler: Merging stage rdd profiles: Set()
23/11/10 17:21:08 DEBUG DAGScheduler: Merging stage rdd profiles: Set()
23/11/10 17:21:08 INFO DAGScheduler: Got job 1 (collect at Bug1.scala:18) with 1 output partitions
23/11/10 17:21:08 INFO DAGScheduler: Final stage: ResultStage 2 (collect at Bug1.scala:18)
23/11/10 17:21:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
23/11/10 17:21:08 INFO DAGScheduler: Missing parents: List()
23/11/10 17:21:08 DEBUG DAGScheduler: submitStage(ResultStage 2 (name=collect at Bug1.scala:18;jobs=1))
23/11/10 17:21:08 DEBUG DAGScheduler: missing: List()
23/11/10 17:21:08 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at collect at Bug1.scala:18), which has no missing parents
23/11/10 17:21:08 DEBUG DAGScheduler: submitMissingTasks(ResultStage 2)
23/11/10 17:21:08 DEBUG DFSClient: WriteChunk allocating new packet seqno=16, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=130560, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:08 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=137942, lastFlushOffset=130770, createNewBlock=false
23/11/10 17:21:08 DEBUG DataStreamer: Queued packet seqno: 16 offsetInBlock: 130560 lastPacketInBlock: false lastByteOffsetInBlock: 137942, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 16
23/11/10 17:21:08 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:08 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 16 offsetInBlock: 130560 lastPacketInBlock: false lastByteOffsetInBlock: 137942
23/11/10 17:21:08 DEBUG DataStreamer: DFSClient seqno: 16 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 trying to put broadcast_1
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 trying to acquire write lock for broadcast_1
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 acquired write lock for broadcast_1
23/11/10 17:21:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 27.6 KiB, free 366.2 MiB)
23/11/10 17:21:08 DEBUG BlockManager: Put block broadcast_1 locally took 0 ms
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 releasing lock for broadcast_1
23/11/10 17:21:08 DEBUG BlockManager: Putting block broadcast_1 without replication took 0 ms
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 trying to put broadcast_1_piece0
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 trying to acquire write lock for broadcast_1_piece0
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 acquired write lock for broadcast_1_piece0
23/11/10 17:21:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 366.2 MiB)
23/11/10 17:21:08 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, Jiahao, 37589, None)
23/11/10 17:21:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on Jiahao:37589 (size: 12.8 KiB, free: 366.3 MiB)
23/11/10 17:21:08 DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0
23/11/10 17:21:08 DEBUG BlockManager: Told master about block broadcast_1_piece0
23/11/10 17:21:08 DEBUG BlockManager: Put block broadcast_1_piece0 locally took 0 ms
23/11/10 17:21:08 TRACE BlockInfoManager: Task -1024 releasing lock for broadcast_1_piece0
23/11/10 17:21:08 DEBUG BlockManager: Putting block broadcast_1_piece0 without replication took 0 ms
23/11/10 17:21:08 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
23/11/10 17:21:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at collect at Bug1.scala:18) (first 15 tasks are for partitions Vector(0))
23/11/10 17:21:08 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
23/11/10 17:21:08 DEBUG TaskSetManager: Epoch for TaskSet 2.0: 1
23/11/10 17:21:08 DEBUG TaskSetManager: Adding pending tasks took 0 ms
23/11/10 17:21:08 DEBUG TaskSetManager: Valid locality levels for TaskSet 2.0: NODE_LOCAL, ANY
23/11/10 17:21:08 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2.0, runningTasks: 0
23/11/10 17:21:08 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (Jiahao, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()
23/11/10 17:21:08 DEBUG TaskSetManager: No tasks for locality level NODE_LOCAL, so moving to locality level ANY
23/11/10 17:21:08 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)
23/11/10 17:21:08 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 1
23/11/10 17:21:08 DEBUG BlockManager: Getting local block broadcast_1
23/11/10 17:21:08 TRACE BlockInfoManager: Task 3 trying to acquire read lock for broadcast_1
23/11/10 17:21:08 TRACE BlockInfoManager: Task 3 acquired read lock for broadcast_1
23/11/10 17:21:08 DEBUG BlockManager: Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
23/11/10 17:21:09 DEBUG MapOutputTrackerMaster: Fetching outputs for shuffle 0
23/11/10 17:21:09 DEBUG MapOutputTrackerMaster: Convert map statuses for shuffle 0, mappers 0-3, partitions 0-200
23/11/10 17:21:09 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
23/11/10 17:21:09 INFO ShuffleBlockFetcherIterator: Getting 3 (180.0 B) non-empty blocks including 3 (180.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
23/11/10 17:21:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
23/11/10 17:21:09 DEBUG ShuffleBlockFetcherIterator: Start fetching local blocks: (shuffle_0_0_43_44,0), (shuffle_0_1_174_175,1), (shuffle_0_2_51_52,2)
23/11/10 17:21:09 DEBUG BlockManager: Getting local shuffle block shuffle_0_0_43_44
23/11/10 17:21:09 DEBUG BlockManager: Getting local shuffle block shuffle_0_1_174_175
23/11/10 17:21:09 DEBUG BlockManager: Getting local shuffle block shuffle_0_2_51_52
23/11/10 17:21:09 DEBUG ShuffleBlockFetcherIterator: Got local blocks in 7 ms
23/11/10 17:21:09 DEBUG GenerateUnsafeProjection: code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

23/11/10 17:21:09 TRACE package$ExpressionCanonicalizer: Fixed point reached for batch CleanExpressions after 1 iterations.
23/11/10 17:21:09 TRACE PlanChangeLogger: Batch CleanExpressions has no effect.
23/11/10 17:21:09 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 5.869E-6 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:09 DEBUG GenerateUnsafeProjection: code for input[0, int, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     int value_0 = isNull_0 ?
/* 033 */     -1 : (i.getInt(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

23/11/10 17:21:09 DEBUG TaskMemoryManager: Task 3 acquired 256.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@2f3feb8f
23/11/10 17:21:09 TRACE TaskMemoryManager: Allocate page number 0 (262144 bytes)
23/11/10 17:21:09 DEBUG GenerateUnsafeProjection: code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

23/11/10 17:21:09 DEBUG TaskMemoryManager: Task 3 acquired 1024.0 KiB for org.apache.spark.unsafe.map.BytesToBytesMap@2f3feb8f
23/11/10 17:21:09 TRACE TaskMemoryManager: Allocate page number 1 (1048576 bytes)
23/11/10 17:21:09 TRACE TaskMemoryManager: Freed page number 0 (262144 bytes)
23/11/10 17:21:09 DEBUG TaskMemoryManager: Task 3 release 256.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@2f3feb8f
23/11/10 17:21:09 TRACE TaskMemoryManager: Freed page number 1 (1048576 bytes)
23/11/10 17:21:09 DEBUG TaskMemoryManager: Task 3 release 1024.0 KiB from org.apache.spark.unsafe.map.BytesToBytesMap@2f3feb8f
23/11/10 17:21:09 TRACE BlockInfoManager: Task 3 releasing lock for broadcast_1
23/11/10 17:21:09 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 3571 bytes result sent to driver
23/11/10 17:21:09 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 0
23/11/10 17:21:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 71 ms on Jiahao (executor driver) (1/1)
23/11/10 17:21:09 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
23/11/10 17:21:09 INFO DAGScheduler: ResultStage 2 (collect at Bug1.scala:18) finished in 0.079 s
23/11/10 17:21:09 DEBUG DFSClient: WriteChunk allocating new packet seqno=17, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=137728, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:09 DEBUG DAGScheduler: After removal of stage 2, remaining stages = 1
23/11/10 17:21:09 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0
23/11/10 17:21:09 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=152458, lastFlushOffset=137942, createNewBlock=false
23/11/10 17:21:09 DEBUG DataStreamer: Queued packet seqno: 17 offsetInBlock: 137728 lastPacketInBlock: false lastByteOffsetInBlock: 152458, blk_1073742676_1857
23/11/10 17:21:09 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 17
23/11/10 17:21:09 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:09 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/10 17:21:09 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 17 offsetInBlock: 137728 lastPacketInBlock: false lastByteOffsetInBlock: 152458
23/11/10 17:21:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
23/11/10 17:21:09 DEBUG DataStreamer: DFSClient seqno: 17 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:09 DEBUG DFSClient: WriteChunk allocating new packet seqno=18, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=152064, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:09 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=152572, lastFlushOffset=152458, createNewBlock=false
23/11/10 17:21:09 DEBUG DataStreamer: Queued packet seqno: 18 offsetInBlock: 152064 lastPacketInBlock: false lastByteOffsetInBlock: 152572, blk_1073742676_1857
23/11/10 17:21:09 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 18
23/11/10 17:21:09 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:09 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 18 offsetInBlock: 152064 lastPacketInBlock: false lastByteOffsetInBlock: 152572
23/11/10 17:21:09 INFO DAGScheduler: Job 1 finished: collect at Bug1.scala:18, took 0.090564 s
23/11/10 17:21:09 DEBUG DataStreamer: DFSClient seqno: 18 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:09 DEBUG AdaptiveSparkPlanExec: Final plan: *(2) HashAggregate(keys=[id#4], functions=[], output=[id#6])
+- AQEShuffleRead coalesced
   +- ShuffleQueryStage 0
      +- Exchange hashpartitioning(id#4, 200), ENSURE_REQUIREMENTS, [id=#22]
         +- *(1) HashAggregate(keys=[id#4], functions=[], output=[id#4])
            +- *(1) LocalTableScan [id#4]

23/11/10 17:21:09 TRACE package$ExpressionCanonicalizer: Fixed point reached for batch CleanExpressions after 1 iterations.
23/11/10 17:21:09 TRACE PlanChangeLogger: Batch CleanExpressions has no effect.
23/11/10 17:21:09 TRACE PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 1.4878E-5 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
23/11/10 17:21:09 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, int, false], StructField(id,IntegerType,false)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[1];
/* 024 */
/* 025 */     int value_1 = i.getInt(0);
/* 026 */     if (false) {
/* 027 */       values_0[0] = null;
/* 028 */     } else {
/* 029 */       values_0[0] = value_1;
/* 030 */     }
/* 031 */
/* 032 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 033 */     if (false) {
/* 034 */       mutableRow.setNullAt(0);
/* 035 */     } else {
/* 036 */
/* 037 */       mutableRow.update(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     return mutableRow;
/* 041 */   }
/* 042 */
/* 043 */
/* 044 */ }

23/11/10 17:21:09 DEBUG CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[1];
/* 024 */
/* 025 */     int value_1 = i.getInt(0);
/* 026 */     if (false) {
/* 027 */       values_0[0] = null;
/* 028 */     } else {
/* 029 */       values_0[0] = value_1;
/* 030 */     }
/* 031 */
/* 032 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 033 */     if (false) {
/* 034 */       mutableRow.setNullAt(0);
/* 035 */     } else {
/* 036 */
/* 037 */       mutableRow.update(0, value_0);
/* 038 */     }
/* 039 */
/* 040 */     return mutableRow;
/* 041 */   }
/* 042 */
/* 043 */
/* 044 */ }

23/11/10 17:21:09 INFO CodeGenerator: Code generated in 5.395637 ms
[RESULT]
23/11/10 17:21:09 DEBUG DFSClient: WriteChunk allocating new packet seqno=19, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=152064, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:09 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=152684, lastFlushOffset=152572, createNewBlock=false
23/11/10 17:21:09 DEBUG DataStreamer: Queued packet seqno: 19 offsetInBlock: 152064 lastPacketInBlock: false lastByteOffsetInBlock: 152684, blk_1073742676_1857
23/11/10 17:21:09 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 19
23/11/10 17:21:09 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:09 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 19 offsetInBlock: 152064 lastPacketInBlock: false lastByteOffsetInBlock: 152684
23/11/10 17:21:09 DEBUG DataStreamer: DFSClient seqno: 19 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
Array([2], [3], [4])
3
Completed
23/11/10 17:21:09 INFO SparkContext: Invoking stop() from shutdown hook
23/11/10 17:21:09 DEBUG DFSClient: WriteChunk allocating new packet seqno=20, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=152576, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:09 DEBUG DFSClient: DFSClient flush():  bytesCurBlock=152750, lastFlushOffset=152684, createNewBlock=false
23/11/10 17:21:09 DEBUG DataStreamer: Queued packet seqno: 20 offsetInBlock: 152576 lastPacketInBlock: false lastByteOffsetInBlock: 152750, blk_1073742676_1857
23/11/10 17:21:09 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 20
23/11/10 17:21:09 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:09 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 20 offsetInBlock: 152576 lastPacketInBlock: false lastByteOffsetInBlock: 152750
23/11/10 17:21:09 DEBUG DataStreamer: DFSClient seqno: 20 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:09 INFO SparkUI: Stopped Spark web UI at http://Jiahao:4040
23/11/10 17:21:09 DEBUG DFSClient: WriteChunk allocating new packet seqno=21, src=/spark3.3.0-logs/local-1699608064198.inprogress, packetSize=65016, chunksPerPacket=126, bytesCurBlock=152576, output stream=DFSOutputStream:blk_1073742676_1857
23/11/10 17:21:09 DEBUG DataStreamer: Queued packet seqno: 21 offsetInBlock: 152576 lastPacketInBlock: false lastByteOffsetInBlock: 152750, blk_1073742676_1857
23/11/10 17:21:09 DEBUG DataStreamer: Queued packet seqno: 22 offsetInBlock: 152750 lastPacketInBlock: true lastByteOffsetInBlock: 152750, blk_1073742676_1857
23/11/10 17:21:09 DEBUG DataStreamer: blk_1073742676_1857 waiting for ack for: 22
23/11/10 17:21:09 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:09 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 21 offsetInBlock: 152576 lastPacketInBlock: false lastByteOffsetInBlock: 152750
23/11/10 17:21:09 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073742676_1857
23/11/10 17:21:09 DEBUG DataStreamer: DFSClient seqno: 21 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:09 DEBUG DataStreamer: blk_1073742676_1857 sending packet seqno: 22 offsetInBlock: 152750 lastPacketInBlock: true lastByteOffsetInBlock: 152750
23/11/10 17:21:09 DEBUG DataStreamer: DFSClient seqno: 22 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0
23/11/10 17:21:09 DEBUG DataStreamer: Closing old block BP-80912574-127.0.1.1-1665050837178:blk_1073742676_1857
23/11/10 17:21:09 TRACE ProtobufRpcEngine2: 101: Call -> Jiahao/127.0.1.1:9000: complete {src: "/spark3.3.0-logs/local-1699608064198.inprogress" clientName: "DFSClient_NONMAPREDUCE_-784229600_1" last { poolId: "BP-80912574-127.0.1.1-1665050837178" blockId: 1073742676 generationStamp: 1857 numBytes: 152750 } fileId: 17316}
23/11/10 17:21:09 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup sending #7 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete
23/11/10 17:21:09 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup got value #7
23/11/10 17:21:09 DEBUG ProtobufRpcEngine2: Call: complete took 2ms
23/11/10 17:21:09 TRACE ProtobufRpcEngine2: 101: Response <- Jiahao/127.0.1.1:9000: complete {result: true}
23/11/10 17:21:09 TRACE ProtobufRpcEngine2: 101: Call -> Jiahao/127.0.1.1:9000: getFileInfo {src: "/spark3.3.0-logs/local-1699608064198"}
23/11/10 17:21:09 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup sending #8 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo
23/11/10 17:21:09 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup got value #8
23/11/10 17:21:09 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms
23/11/10 17:21:09 TRACE ProtobufRpcEngine2: 101: Response <- Jiahao/127.0.1.1:9000: getFileInfo {}
23/11/10 17:21:09 TRACE ProtobufRpcEngine2: 101: Call -> Jiahao/127.0.1.1:9000: rename {src: "/spark3.3.0-logs/local-1699608064198.inprogress" dst: "/spark3.3.0-logs/local-1699608064198"}
23/11/10 17:21:09 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup sending #9 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename
23/11/10 17:21:09 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup got value #9
23/11/10 17:21:09 DEBUG ProtobufRpcEngine2: Call: rename took 2ms
23/11/10 17:21:09 TRACE ProtobufRpcEngine2: 101: Response <- Jiahao/127.0.1.1:9000: rename {result: true}
23/11/10 17:21:09 TRACE ProtobufRpcEngine2: 101: Call -> Jiahao/127.0.1.1:9000: setTimes {src: "/spark3.3.0-logs/local-1699608064198" mtime: 1699608069084 atime: 18446744073709551615}
23/11/10 17:21:09 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup sending #10 org.apache.hadoop.hdfs.protocol.ClientProtocol.setTimes
23/11/10 17:21:09 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup got value #10
23/11/10 17:21:09 DEBUG ProtobufRpcEngine2: Call: setTimes took 2ms
23/11/10 17:21:09 TRACE ProtobufRpcEngine2: 101: Response <- Jiahao/127.0.1.1:9000: setTimes {}
23/11/10 17:21:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/10 17:21:09 INFO MemoryStore: MemoryStore cleared
23/11/10 17:21:09 INFO BlockManager: BlockManager stopped
23/11/10 17:21:09 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/10 17:21:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/10 17:21:09 INFO SparkContext: Successfully stopped SparkContext
23/11/10 17:21:09 INFO ShutdownHookManager: Shutdown hook called
23/11/10 17:21:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-721b8c68-8a0a-4bfd-8096-ac9fdb2bf9d5
23/11/10 17:21:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-d66f7738-29c1-4a72-86f6-041587eb6520
23/11/10 17:21:09 DEBUG FileSystem: FileSystem.close() by method: org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:1518)); Key: (dbgroup (auth:SIMPLE))@hdfs://jiahao:9000; URI: hdfs://Jiahao:9000; Object Identity Hash: 4899df75
23/11/10 17:21:09 TRACE FileSystem: FileSystem.close() full stack trace:
java.lang.Throwable
	at org.apache.hadoop.fs.FileSystem.debugLogFileSystemClose(FileSystem.java:645)
	at org.apache.hadoop.fs.FileSystem.close(FileSystem.java:2592)
	at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:1518)
	at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:3678)
	at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:3695)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
23/11/10 17:21:09 DEBUG Client: stopping client from cache: Client-ad2a875726b643d9bb1d0cbc48609258
23/11/10 17:21:09 DEBUG Client: removing client from cache: Client-ad2a875726b643d9bb1d0cbc48609258
23/11/10 17:21:09 DEBUG Client: stopping actual client because no more references remain: Client-ad2a875726b643d9bb1d0cbc48609258
23/11/10 17:21:09 DEBUG Client: Stopping client
23/11/10 17:21:09 TRACE Client: Interrupted while waiting to retrieve RPC response.
23/11/10 17:21:09 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup: closed
23/11/10 17:21:09 DEBUG Client: IPC Client (435034854) connection to Jiahao/127.0.1.1:9000 from dbgroup: stopped, remaining connections 0
23/11/10 17:21:09 DEBUG ShutdownHookManager: Completed shutdown in 0.054 seconds; Timeouts: 0
23/11/10 17:21:09 DEBUG ShutdownHookManager: ShutdownHookManager completed shutdown.
